{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmlDC/MediaBias-Thesis22-23/blob/Developing-Tool/Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MEDIA BIAS Thesis"
      ],
      "metadata": {
        "id": "5UbRZ1dHzH2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Environment"
      ],
      "metadata": {
        "id": "9DnFpdsboI45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PIP install"
      ],
      "metadata": {
        "id": "Pk8TPLKJoaiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install requests-html\n",
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip3 install selenium\n",
        "!pip3 install webdriver-manager"
      ],
      "metadata": {
        "id": "qhBA8KY9oipI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b265c192-3b69-48a6-b2d8-15551c6657d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.27.1)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parse (from requests-html)\n",
            "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bs4 (from requests-html)\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2021 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (2022.12.7)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (6.6.0)\n",
            "Collecting pyee<9.0.0,>=8.1.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.65.0)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.26.15)\n",
            "Collecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests-html) (4.11.2)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests-html) (4.9.2)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html) (3.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests-html) (2.4.1)\n",
            "Building wheels for collected packages: bs4, parse\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1270 sha256=b29c52060253f1a8ed7cbc972c9cc43024886b16196292f4cf92c99106a669c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24589 sha256=d873ce7219a83bc225af0522e559206aa721306cf56c70b2f9ddcb373d645109\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4b/f0/eaf5a8de646d8676dc25caa01949b9f9d883b8fa2efb435bc3\n",
            "Successfully built bs4 parse\n",
            "Installing collected packages: pyee, parse, fake-useragent, websockets, w3lib, cssselect, pyquery, pyppeteer, bs4, requests-html\n",
            "Successfully installed bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.3 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-2.0.0 requests-html-0.10.0 w3lib-2.1.1 websockets-10.4\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ Packages [77.6 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,039 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,668 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,334 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,150 kB]\n",
            "Fetched 8,607 kB in 8s (1,041 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "27 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser liblzo2-2 libudev1 snapd squashfs-tools udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver liblzo2-2 snapd\n",
            "  squashfs-tools udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 7 newly installed, 0 to remove and 26 not upgraded.\n",
            "Need to get 40.0 MB of archives.\n",
            "After this operation, 183 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 apparmor amd64 2.13.3-7ubuntu5.2 [502 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 liblzo2-2 amd64 2.10-2 [50.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 squashfs-tools amd64 1:4.4-1ubuntu0.3 [117 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libudev1 amd64 245.4-4ubuntu3.21 [75.9 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 udev amd64 245.4-4ubuntu3.21 [1,366 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 snapd amd64 2.58+20.04 [37.9 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu0.20.04.3 [48.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu0.20.04.3 [2,496 B]\n",
            "Fetched 40.0 MB in 3s (15.4 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_2.13.3-7ubuntu5.2_amd64.deb ...\n",
            "Unpacking apparmor (2.13.3-7ubuntu5.2) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.4-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.4-1ubuntu0.3) ...\n",
            "Preparing to unpack .../libudev1_245.4-4ubuntu3.21_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (245.4-4ubuntu3.21) over (245.4-4ubuntu3.19) ...\n",
            "Setting up libudev1:amd64 (245.4-4ubuntu3.21) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 122705 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_245.4-4ubuntu3.21_amd64.deb ...\n",
            "Unpacking udev (245.4-4ubuntu3.21) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.58+20.04_amd64.deb ...\n",
            "Unpacking snapd (2.58+20.04) ...\n",
            "Setting up apparmor (2.13.3-7ubuntu5.2) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2) ...\n",
            "Setting up squashfs-tools (1:4.4-1ubuntu0.3) ...\n",
            "Setting up udev (245.4-4ubuntu3.21) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up snapd (2.58+20.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.aa-prompt-listener.service → /lib/systemd/system/snapd.aa-prompt-listener.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 122926 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu0.20.04.3_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu0.20.04.3) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu0.20.04.3_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu0.20.04.3) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu0.20.04.3) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu0.20.04.3) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for systemd (245.4-4ubuntu3.21) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.9.0-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]~=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.26.15)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.10.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2022.12.7)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting async-generator>=1.9 (from trio~=0.17->selenium)\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, async-generator, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed async-generator-1.10 h11-0.14.0 outcome-1.2.0 selenium-4.9.0 trio-0.22.0 trio-websocket-0.10.2 wsproto-1.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-3.8.6-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (2.27.1)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (4.65.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (3.4)\n",
            "Installing collected packages: python-dotenv, webdriver-manager\n",
            "Successfully installed python-dotenv-1.0.0 webdriver-manager-3.8.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Needed for Selenium\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAD7RaaP1Z3s",
        "outputId": "e103a46b-6d72-4cb3-9fe0-ce0954ef7a00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "4BDsarMioPoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ],
      "metadata": {
        "id": "kDHMeE4S1i2s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uWuhmODroQyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iTwWZbPg1TVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bedeb9e-a112-46be-d891-1f8e4256c981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trio/_core/_multierror.py:406: RuntimeWarning: IPython detected, but you already have a custom exception handler installed. I'll skip installing Trio's custom handler, but this means exception groups will not show full tracebacks.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import urllib.request,sys,time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "from requests_html import HTMLSession \n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting to GDrive"
      ],
      "metadata": {
        "id": "fygyA3BsoSRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "H7uTO8KVI0HL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106ddc86-bc64-42f0-e87a-edc603a01550"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dir variable"
      ],
      "metadata": {
        "id": "_CJm56YhoVDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir  = \"/content/gdrive/MyDrive/THESIS-MS/Git-Thesis22-23/\""
      ],
      "metadata": {
        "id": "FcAOB3l1JQD9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping"
      ],
      "metadata": {
        "id": "8IhlJmBFI7YI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Source Function"
      ],
      "metadata": {
        "id": "386CSzX1nloW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_source(url):\n",
        "    agent = {\"User-Agent\":\"Chrome/105.0.0.0\"}\n",
        "    try:\n",
        "      source=requests.get(url, headers=agent)\n",
        "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "      error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "      print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
        "      print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
        "    \n",
        "    return source                                               #ignore this page. Abandon this and go back."
      ],
      "metadata": {
        "id": "pWGtJbU6MOyL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Politifact"
      ],
      "metadata": {
        "id": "YAbVIXQvngVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Politifact \n",
        "pagesToGet= 1\n",
        "upperframe=[]  \n",
        "site = \"politifact\"\n",
        "\n",
        "\n",
        "for page in range(1,pagesToGet+1):\n",
        "    print('processing page :', page)\n",
        "    url = 'https://www.politifact.com/factchecks/list/?page='+str(page)\n",
        "    print(url)\n",
        "    \n",
        "    #an exception might be thrown, so the code should be in a try-except block\n",
        "    try:\n",
        "        #use the browser to get the url. This is suspicious command that might blow up.\n",
        "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
        "    \n",
        "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
        "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
        "        continue                                              #ignore this page. Abandon this and go back.\n",
        "\n",
        "    time.sleep(2)   \n",
        "    soup=BeautifulSoup(page.text,'html.parser')\n",
        "    frame=[]\n",
        "    links=soup.find_all('li',attrs={'class':'o-listicle__item'})\n",
        "    print(len(links))\n",
        "\n",
        "    \n",
        "    filename= f\"{dir + site}NEWS.csv\"\n",
        "    f=open(filename,\"w\", encoding = 'utf-8')\n",
        "    headers=\"Statement,Link,Date, Source, Label\\n\"\n",
        "    f.write(headers)\n",
        "    \n",
        "    for j in links:\n",
        "        Statement = j.find(\"div\",attrs={'class':'m-statement__quote'}).text.strip()\n",
        "        Link = \"https://www.politifact.com\"\n",
        "        Link += j.find(\"div\",attrs={'class':'m-statement__quote'}).find('a')['href'].strip()\n",
        "        Date = j.find('div',attrs={'class':'m-statement__body'}).find('footer').text[-14:-1].strip()\n",
        "        Source = j.find('div', attrs={'class':'m-statement__meta'}).find('a').text.strip()\n",
        "        Label = j.find('div', attrs ={'class':'m-statement__content'}).find('img',attrs={'class':'c-image__original'}).get('alt').strip()\n",
        "        frame.append((Statement,Link,Date,Source,Label))\n",
        "        f.write(Statement.replace(\",\",\"^\")+\",\"+Link+\",\"+Date.replace(\",\",\"^\")+\",\"+Source.replace(\",\",\"^\")+\",\"+Label.replace(\",\",\"^\")+\"\\n\")\n",
        "    upperframe.extend(frame)\n",
        "f.close()\n",
        "data=pd.DataFrame(upperframe, columns=['Statement','Link','Date','Source','Label'])\n",
        "data.head()"
      ],
      "metadata": {
        "id": "BdgXD6mX2uhc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "f10332ab-f9d7-482a-afe7-654486936237"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing page : 1\n",
            "https://www.politifact.com/factchecks/list/?page=1\n",
            "30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           Statement  \\\n",
              "0  “Anheuser-Busch closing half its U.S. brewerie...   \n",
              "1  There is “$500 billion of unspent COVID money”...   \n",
              "2  “Joe Biden was older on his first day as presi...   \n",
              "3  “El nuevo WhatsApp de 2023” permite “ver con q...   \n",
              "4  New York Gov. Kathy Hochul wants “quarantine c...   \n",
              "\n",
              "                                                Link           Date  \\\n",
              "0  https://www.politifact.com/factchecks/2023/may...  • May 1, 2023   \n",
              "1  https://www.politifact.com/factchecks/2023/may...  • May 1, 2023   \n",
              "2  https://www.politifact.com/factchecks/2023/may...  • May 1, 2023   \n",
              "3  https://www.politifact.com/factchecks/2023/may...  • May 1, 2023   \n",
              "4  https://www.politifact.com/factchecks/2023/apr...  pril 29, 2023   \n",
              "\n",
              "            Source       Label  \n",
              "0   Facebook posts  pants-fire  \n",
              "1      Nikki Haley   half-true  \n",
              "2  Jonathan Martin        true  \n",
              "3   Facebook posts       false  \n",
              "4   Facebook posts  pants-fire  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-64d2ccec-5ad9-4fa1-a32c-b3436d809292\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Statement</th>\n",
              "      <th>Link</th>\n",
              "      <th>Date</th>\n",
              "      <th>Source</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>“Anheuser-Busch closing half its U.S. brewerie...</td>\n",
              "      <td>https://www.politifact.com/factchecks/2023/may...</td>\n",
              "      <td>• May 1, 2023</td>\n",
              "      <td>Facebook posts</td>\n",
              "      <td>pants-fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>There is “$500 billion of unspent COVID money”...</td>\n",
              "      <td>https://www.politifact.com/factchecks/2023/may...</td>\n",
              "      <td>• May 1, 2023</td>\n",
              "      <td>Nikki Haley</td>\n",
              "      <td>half-true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>“Joe Biden was older on his first day as presi...</td>\n",
              "      <td>https://www.politifact.com/factchecks/2023/may...</td>\n",
              "      <td>• May 1, 2023</td>\n",
              "      <td>Jonathan Martin</td>\n",
              "      <td>true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>“El nuevo WhatsApp de 2023” permite “ver con q...</td>\n",
              "      <td>https://www.politifact.com/factchecks/2023/may...</td>\n",
              "      <td>• May 1, 2023</td>\n",
              "      <td>Facebook posts</td>\n",
              "      <td>false</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>New York Gov. Kathy Hochul wants “quarantine c...</td>\n",
              "      <td>https://www.politifact.com/factchecks/2023/apr...</td>\n",
              "      <td>pril 29, 2023</td>\n",
              "      <td>Facebook posts</td>\n",
              "      <td>pants-fire</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64d2ccec-5ad9-4fa1-a32c-b3436d809292')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-64d2ccec-5ad9-4fa1-a32c-b3436d809292 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-64d2ccec-5ad9-4fa1-a32c-b3436d809292');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manila Bulletin"
      ],
      "metadata": {
        "id": "Hi1eB--NnqyY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cAfjwAudx_NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Manila bulletin\n",
        "pagesToGet= 1\n",
        "upperframe=[]  \n",
        "topic = \"ukraine war\"\n",
        "site = \"manilaBulletin\"\n",
        "\n",
        "data=pd.DataFrame(columns=['Statement','Link','Date',\"Topic\"])\n",
        "\n",
        "\n",
        "for page in range(1,pagesToGet+1):\n",
        "    print('processing page :', page)\n",
        "    #  https://mb.com.ph/search-results?s=ukraine%20war\n",
        "    url = 'https://mb.com.ph/search-results/?s='+topic.replace(\" \", \"%20\")\n",
        "    print(url)\n",
        "\n",
        "    time.sleep(2)   \n",
        "    soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "\n",
        "    links=soup.find(\"div\", {'class' : 'container pr-0 pl-0 max-width'}).find_all('div',attrs={'class':'row mb-16'})\n",
        "    print(len(links))\n",
        "    \n",
        "    filename= f\"{dir}{site}-{topic}-NEWS-List.csv\"    \n",
        "    index = 0\n",
        "    for j in links:\n",
        "        Statement = j.find('h4', {'class' : 'mb-font-article-title'}).find('a').text.strip()\n",
        "        Link = j.find(\"h4\", {'class' : 'mb-font-article-title'}).find('a')['href'].strip()\n",
        "        Date = j.find('div', {'class': 'ml-1 mb-font-article-date'}).text.strip()\n",
        "        df_new_row = { 'Statement': Statement, 'Link': Link, 'Date': Date, 'Topic': topic }\n",
        "        index += 1\n",
        "        data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
        "        # pd.concat([data, pd.DataFrame((df_new_row), index=[index])], ignore_index = True)\n",
        "data.to_csv(filename)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "qQegsdzKBVWv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "5d21c26e-acfa-406e-f100-e566166813fc"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing page : 1\n",
            "https://mb.com.ph/search-results/?s=ukraine%20war\n",
            "10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-63-d33300d98d44>:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
            "<ipython-input-63-d33300d98d44>:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
            "<ipython-input-63-d33300d98d44>:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
            "<ipython-input-63-d33300d98d44>:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
            "<ipython-input-63-d33300d98d44>:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
            "<ipython-input-63-d33300d98d44>:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
            "<ipython-input-63-d33300d98d44>:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
            "<ipython-input-63-d33300d98d44>:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
            "<ipython-input-63-d33300d98d44>:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
            "<ipython-input-63-d33300d98d44>:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           Statement  \\\n",
              "0  AI chatbots have been used to create dozens of...   \n",
              "1  UN chief, envoys in key talks on Afghanistan c...   \n",
              "2  China FM warns of 'dangerous consequences' of ...   \n",
              "3  Brazil's Lula visits Europe amid row over Ukra...   \n",
              "4                  ICTSI reports record year in 2022   \n",
              "\n",
              "                                                Link         Date        Topic  \n",
              "0  /2023/5/1/ai-chatbots-have-been-used-to-create...    a day ago  ukraine war  \n",
              "1  /2023/4/30/un-chief-envoys-in-key-talks-on-afg...   2 days ago  ukraine war  \n",
              "2  /2023/4/21/china-fm-warns-of-dangerous-consequ...  11 days ago  ukraine war  \n",
              "3  /2023/4/21/brazil-s-lula-visits-europe-amid-ro...  11 days ago  ukraine war  \n",
              "4  /2023/4/20/razon-2022-a-record-year-for-ictsi-...  12 days ago  ukraine war  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42cc2f7b-c80f-4b0c-b3d0-c02c45c77fb2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Statement</th>\n",
              "      <th>Link</th>\n",
              "      <th>Date</th>\n",
              "      <th>Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AI chatbots have been used to create dozens of...</td>\n",
              "      <td>/2023/5/1/ai-chatbots-have-been-used-to-create...</td>\n",
              "      <td>a day ago</td>\n",
              "      <td>ukraine war</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UN chief, envoys in key talks on Afghanistan c...</td>\n",
              "      <td>/2023/4/30/un-chief-envoys-in-key-talks-on-afg...</td>\n",
              "      <td>2 days ago</td>\n",
              "      <td>ukraine war</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>China FM warns of 'dangerous consequences' of ...</td>\n",
              "      <td>/2023/4/21/china-fm-warns-of-dangerous-consequ...</td>\n",
              "      <td>11 days ago</td>\n",
              "      <td>ukraine war</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Brazil's Lula visits Europe amid row over Ukra...</td>\n",
              "      <td>/2023/4/21/brazil-s-lula-visits-europe-amid-ro...</td>\n",
              "      <td>11 days ago</td>\n",
              "      <td>ukraine war</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ICTSI reports record year in 2022</td>\n",
              "      <td>/2023/4/20/razon-2022-a-record-year-for-ictsi-...</td>\n",
              "      <td>12 days ago</td>\n",
              "      <td>ukraine war</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42cc2f7b-c80f-4b0c-b3d0-c02c45c77fb2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-42cc2f7b-c80f-4b0c-b3d0-c02c45c77fb2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-42cc2f7b-c80f-4b0c-b3d0-c02c45c77fb2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "filename= f\"{dir+site+'-'+topic}NEWS.csv\"\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  url = row[\"Link\"] \n",
        "  print(url)\n",
        "\n",
        "  time.sleep(1)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "\n",
        "  head=soup.find(\"article\", {'id' : 'the-article-content'})\n",
        "  # print(content.prettify())\n",
        "  textList =soup.find(\"article\", {'id' : 'the-article-content'}).find(\"section\", {'class': 'article-content'}).find_all(\"p\")\n",
        "  \n",
        "  \n",
        "  row=[]\n",
        "  try:\n",
        "    Title = head.find('h2',{'class':'title'}).text.strip()\n",
        "    Author = head.find('p',attrs={'class':'author'}).find('a').text[3:].strip()\n",
        "    Date = head.find('p', attrs={'class': 'published'}).text[10:-9].strip(\",\") \n",
        "  except Exception as e:\n",
        "      pass\n",
        "  \n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    body += (t.text.replace('\\u200b', '')) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "\n",
        "  data2 = data2.append(pd.Series(row, index=data2.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "data2.to_csv(filename)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "5Vt_oQKsLB7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ABS CBN"
      ],
      "metadata": {
        "id": "00MJ5PJZn6M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ABS CBN\n",
        "pagesToGet= 2\n",
        "topic = \"Queen Elizabeth Death\"\n",
        "site = \"ABS-CBN_NEWS\"\n",
        "\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "userAgent = \"Chrome/105.0.0.0\"\n",
        "chrome_options.add_argument(f\"user-agent={userAgent}\")\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "data=pd.DataFrame(columns=['Title','Link','Date',\"Topic\"])\n",
        "\n",
        "for page in range(1,pagesToGet+1):\n",
        "  url = 'https://news.abs-cbn.com/special-pages/search?q='+topic.replace(\" \", \"%20\")+'&gsc.sort=&gsc.page='+str(page)+'#gsc.tab=0&gsc.q='+topic.replace(\" \", \"%20\")+'&gsc.sort=&gsc.page='+str(page)\n",
        "  print(url)\n",
        "\n",
        "  driver.get(url)\n",
        "  # WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.ID, '___gcse_0'))) \n",
        "  source = driver.page_source\n",
        "\n",
        "  time.sleep(2)   \n",
        "  soup = BeautifulSoup(source, 'html.parser')\n",
        "  searches = soup.find_all('div', {'class':'gsc-webResult gsc-result'})\n",
        "  filename= f\"{dir}{site}-{topic}-NEWS-List.csv\"     \n",
        "\n",
        "  for x in searches:\n",
        "    frame=[]\n",
        "    Title = x.find('div', class_=\"gs-title\").find(\"a\").text.strip()\n",
        "    Date = x.find('div', class_=\"gs-bidi-start-align gs-snippet\").text[:12].strip()\n",
        "    Link = x.find('a', class_=\"gs-title\")['href']\n",
        "    frame.extend((Title, Link, Date, topic))\n",
        "\n",
        "    data = data.append(pd.Series(frame, index=data.columns[:len(frame)]), ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "data.to_csv(filename)\n",
        "driver.quit()\n",
        "data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qg8vzExe8qtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "filename= f\"{dir+site+'-'+topic}NEWS.csv\"\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  url = row[\"Link\"] \n",
        "  if ('multimedia/photo' in url):\n",
        "    pass\n",
        "  else:\n",
        "    print(url)\n",
        "\n",
        "    time.sleep(1)   \n",
        "    soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "      \n",
        "    row=[]\n",
        "    \n",
        "    Title = soup.find('h1',{'class':'news-title'}).text.strip()\n",
        "    Author = soup.find('span',attrs={'class':'editor'}).text.strip()\n",
        "    Date = soup.find('span', attrs={'class': 'date-posted'}).text[:-8].strip() \n",
        "    \n",
        "    textList =soup.find(\"div\", {'class' :'article-content'}).find_all(\"p\")\n",
        "    body = \"\"\n",
        "    for t in textList:\n",
        "      if (t.text != \"RELATED VIDEO:\"):\n",
        "        body += (t.text) +\"\\n\" \n",
        "\n",
        "    row.extend((Title, Author, Date, body))\n",
        "    \n",
        "\n",
        "    data2 = data2.append(pd.Series(row, index=data2.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "data2.to_csv(filename)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "YIfu-vRWOzcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMA "
      ],
      "metadata": {
        "id": "WaPofL-jn9u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.gmanetwork.com/news/#/search;query=queen%20elizabeth%20death;sortBy=_score;isDesc=1\n",
        "\n",
        "topic = \"Queen Elizabeth Death\"\n",
        "site = \"GMA-Network\"\n",
        "\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "userAgent = \"Chrome/105.0.0.0\"\n",
        "chrome_options.add_argument(f\"user-agent={userAgent}\")\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "data=pd.DataFrame(columns=['Title','Link','Date',\"Topic\"])\n",
        "\n",
        "url = 'https://www.gmanetwork.com/news/#/search;query='+topic.replace(\" \", \"%20\")+';sortBy=_score;isDesc=1'\n",
        "print(url)\n",
        "\n",
        "driver.get(url)\n",
        "WebDriverWait(driver, 50).until(EC.visibility_of_element_located((By.CLASS_NAME, 'clearfix'))) \n",
        "source = driver.page_source\n",
        "\n",
        "time.sleep(2)   \n",
        "soup = BeautifulSoup(source, 'html.parser')\n",
        "searches = soup.find_all('div', {'class':'zd-row-text-wrap'})\n",
        "filename= f\"{dir}{site}-{topic}-NEWS-List.csv\"     \n",
        "print(len(searches))\n",
        "\n",
        "for x in searches:\n",
        "  frame=[]\n",
        "  Title = x.find('h4', class_=\"zd-result-title ng-star-inserted\").text.strip()\n",
        "  Date = x.find('div', class_=\"zd-result-subTitle ng-star-inserted\").text[:-18].strip()\n",
        "  partialTitle = ' '.join(Title.split()[:8])\n",
        "\n",
        "  try:\n",
        "    Link = \"\"\n",
        "    to_Click = driver.find_element(By.XPATH, f\"//*[contains(text(), '{Title}')]\")\n",
        "    print(to_Click.text)\n",
        "    to_Click.click()\n",
        "    driver.switch_to.window(driver.window_handles[1])\n",
        "    Link = driver.current_url\n",
        "    driver.close()\n",
        "    driver.switch_to.window(driver.window_handles[0])\n",
        "  except Exception as e:\n",
        "    print(\"Something wrong. Error:\", e.split()[0])\n",
        "\n",
        "  frame.extend((Title, Link, Date, topic))\n",
        "\n",
        "  data = data.append(pd.Series(frame, index=data.columns[:len(frame)]), ignore_index=True)\n",
        "\n",
        "data.to_csv(filename)\n",
        "driver.quit()\n",
        "data.head(10)\n"
      ],
      "metadata": {
        "id": "RQ9n3DXbUD4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "rqq5vKh0nHZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "filename= f\"{dir+site+'-'+topic}NEWS.csv\"\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  url = row[\"Link\"] \n",
        "  if (url == \"\"):\n",
        "    continue\n",
        "  \n",
        "  print(url)\n",
        "  time.sleep(2)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "    \n",
        "  row=[]\n",
        "  \n",
        "  Title = soup.find('h1',{'class':'story_links'}).text.strip()\n",
        "  Author = soup.find('div',attrs={'class':'main-byline'}).find('div').find('div').text.strip()\n",
        "  Date = soup.find('div', attrs={'class': 'article-time'}).find('time')['datetime'][:10].strip() \n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'story_main'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    if (t.find('p', class_=\"ad\")):\n",
        "      pass\n",
        "    else:\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "\n",
        "  data2 = data2.append(pd.Series(row, index=data2.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "data2.to_csv(filename)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "yl8f_da2j_yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application"
      ],
      "metadata": {
        "id": "DhFATEIp7J54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Function"
      ],
      "metadata": {
        "id": "AAq7YZnx7E6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def abs_scrape(url):\n",
        "  abs_df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "\n",
        "  time.sleep(1)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "    \n",
        "  row=[]\n",
        "  \n",
        "  Title = soup.find('h1',{'class':'news-title'}).text.strip()\n",
        "  Author = soup.find('span',attrs={'class':'editor'}).text.strip()\n",
        "  Date = soup.find('span', attrs={'class': 'date-posted'}).text[:-8].strip() \n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'article-content'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    if (t.text != \"RELATED VIDEO:\"):\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "  abs_df = abs_df.append(pd.Series(row, index=abs_df.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "  return abs_df"
      ],
      "metadata": {
        "id": "ZYNsYAKK3cpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gma_scrape(url):\n",
        "  gma_df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "  \n",
        "  time.sleep(2)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "    \n",
        "  row=[]\n",
        "\n",
        "  Title = soup.find('h1',{'class':'story_links'}).text.strip()\n",
        "  Author = soup.find('div',attrs={'class':'main-byline'})\n",
        "  if (Author != None):\n",
        "    Author = Author.find('div').find('div').text.strip() \n",
        "  Date = soup.find('div', attrs={'class': 'article-time'}).find('time')['datetime'][:10].strip() \n",
        "\n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'story_main'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    if (t.find('p', class_=\"ad\")):\n",
        "      pass\n",
        "    else:\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "  gma_df = gma_df.append(pd.Series(row, index=gma_df.columns[:len(row)]), ignore_index=True)\n",
        "\n",
        "  return gma_df\n",
        "  \n"
      ],
      "metadata": {
        "id": "rc4Qqqk-7Pi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_scrape(url):\n",
        "\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "  userAgent = \"Chrome/105.0.0.0\"\n",
        "  chrome_options.add_argument(f\"user-agent={userAgent}\")\n",
        "  driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "  driver.get(url)\n",
        "  WebDriverWait(driver, 50).until(EC.visibility_of_element_located((By.CLASS_NAME, 'title'))) \n",
        "  source = driver.page_source\n",
        "\n",
        "  cnn_df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "  \n",
        "  soup=BeautifulSoup(source, 'html.parser')\n",
        "  # time.sleep(2)   \n",
        "\n",
        "  # print(soup)\n",
        "  row=[]\n",
        "  \n",
        "  Title = soup.find('h1',{'class':'title'}).text.strip()\n",
        "  Author = soup.find('div',attrs={'class':'author-byline'}).find('p').find('a').text.strip()\n",
        "  Date = soup.find('div', attrs={'class': 'dateLine'}).find('p', {'class': 'dateString no-icon'}).text[10:-11].strip() \n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'article-maincontent-p cnn-life-body'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "\n",
        "  cnn_df = cnn_df.append(pd.Series(row, index=cnn_df.columns[:len(row)]), ignore_index=True)\n",
        "\n",
        "  return cnn_df"
      ],
      "metadata": {
        "id": "qpXpr6qQkKfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(df, text, clean_col , sw = [] ):\n",
        "    \"\"\"\n",
        "    Remove blank texts, replaces text with lower case characters,\n",
        "    remove special characters, remove other special texts like URLs and Twitter handles, \n",
        "    remove leading and trailing whitespaces, and remove stopwords.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove blank texts\n",
        "    df[clean_col] = df[text].fillna('')     \n",
        "\n",
        "    # Transform into lowercase\n",
        "    df[clean_col] = df[clean_col].str.lower()\n",
        "\n",
        "    # # Remove non-alphanumeric characters\n",
        "    df[clean_col] = df[clean_col].str.replace(r'(@[A-Za-z0-9_]+)|([^A-Za-z0-9_ \\t])|(\\w+:\\/\\/\\S+)', '')\n",
        "    \n",
        "    # # Lemmatize verbs\n",
        "    # df['cleaned_text'] = df['cleaned_text'].apply(lambda row: ' '.join([lemmatizer.lemmatize(x, 'v') for x in row.split()]))\n",
        "\n",
        "    # # Lemmatize adjectives\n",
        "    # df['cleaned_text'] = df['cleaned_text'].apply(lambda row: ' '.join([lemmatizer.lemmatize(x, 'a') for x in row.split()]))\n",
        "\n",
        "    # # Lemmatize nouns\n",
        "    # df['cleaned_text'] = df['cleaned_text'].apply(lambda row: ' '.join([lemmatizer.lemmatize(x, 'n') for x in row.split()]))\n",
        "\n",
        "    # Remove trailing and leading whitespaces\n",
        "    df[clean_col] = df[clean_col].str.replace(r'^\\s+|\\s+$', '')\n",
        "\n",
        "    # # Remove stopwords \n",
        "    df[clean_col] = df[clean_col].apply(lambda row: ' '.join([word for word in row.split() if word not in (stopwords + sw)])) \n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "oAGB3dkZFyo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def senti_Analysis(df, col, colLabel ):\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  newcol = {'compoundScore' : [sia.polarity_scores(entry) for entry in col]}\n",
        "  if (colLabel in df.columns):\n",
        "      df[colLabel] =  newcol['compoundScore']\n",
        "  else:\n",
        "    df.insert(df.shape[1], colLabel, newcol['compoundScore'])\n",
        "  "
      ],
      "metadata": {
        "id": "b-Xwekv-LJOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape"
      ],
      "metadata": {
        "id": "nnlTbHfAQvlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "df_ = df_.append(abs_scrape(\"https://news.abs-cbn.com/news/09/29/22/ovp-eyes-coconut-palace-as-permanent-home\"), ignore_index=True)\n",
        "df_ = df_.append(gma_scrape(\"https://www.gmanetwork.com/news/topstories/nation/846433/sara-duterte-ovp-in-talks-with-gsis-for-possible-acquisition-of-coconut-palace/story/\"), ignore_index=True)\n",
        "df_.head()"
      ],
      "metadata": {
        "id": "rLC433D46a3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Oct 6"
      ],
      "metadata": {
        "id": "pNL6TdVmlv9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "abscbn = abs_scrape('https://news.abs-cbn.com/news/10/04/22/who-funded-marcos-sg-trip-not-relevant-says-bersamin')\n",
        "gma = gma_scrape('https://www.gmanetwork.com/news/topstories/nation/846954/palace-marcos-performed-job-as-president-in-singapore/story/?just_in')\n",
        "cnn = cnn_scrape('https://www.cnnphilippines.com/news/2022/10/4/Marcos-entitled-to-private-time-funding-source-irrelevant.html')\n",
        "\n",
        "df = df.append(abscbn)\n",
        "df = df.append(gma)\n",
        "df = df.append(cnn)\n"
      ],
      "metadata": {
        "id": "5CHCl7HSlz0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addedFilter = ['executive', 'president', 'trip', 'marcos', 'bersamin']\n",
        "visualize((clean(abscbn,\"Text\", \"cleanText\",addedFilter )), \"cleanText\", \"ABS-CBN\")\n",
        "visualize((clean(gma,\"Text\", \"cleanText\",addedFilter)), \"cleanText\", \"GMA\")\n",
        "visualize((clean(cnn,\"Text\", \"cleanText\", addedFilter)), \"cleanText\", \"CNN\")"
      ],
      "metadata": {
        "id": "5BcNpnAOmTVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bDJELKBYpmE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean= clean(df_, 'Text', 'clean_Textbody')\n",
        "df_clean= clean(df_, 'Title', 'clean_Title')\n",
        "df_clean[\"clean_Body_Title\"] = df_clean[\"clean_Title\"].str.cat(df_clean[\"clean_Textbody\"], sep = \"\\n\")\n",
        "df_clean.head()"
      ],
      "metadata": {
        "id": "hA-nRcFf9Bsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def POS(df, dfCol, colLabel):\n",
        "  POS = []\n",
        "  for index, row in df.iterrows():\n",
        "    # print(type(row[dfCol]), row[dfCol])\n",
        "    tokenized = sent_tokenize(row[dfCol])\n",
        "    for i in tokenized:\n",
        "      \n",
        "      # Word tokenizers is used to find the words\n",
        "      # and punctuation in a string\n",
        "      wordsList = nltk.word_tokenize(i)\n",
        "\n",
        "      wordsList = [w for w in wordsList if not w in stopwords]\n",
        "  \n",
        "      #  Using a Tagger. Which is part-of-speech\n",
        "      # tagger or POS-tagger.\n",
        "      tagged = nltk.pos_tag(wordsList)\n",
        "      POS.append(tagged)\n",
        "    df.loc[index, colLabel] =  POS\n",
        "    \n",
        " "
      ],
      "metadata": {
        "id": "Jler8A6aRQk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.geeksforgeeks.org/part-speech-tagging-stop-words-using-nltk-python/\n",
        "POS(df_clean, 'clean_Body_Title', \"POS\")\n",
        "df_clean\n"
      ],
      "metadata": {
        "id": "UwQo58BZSTbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.to_csv(f\"{dir}textCompare.csv\")"
      ],
      "metadata": {
        "id": "Mx6UTXguaEq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}