{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmlDC/MediaBias-Thesis22-23/blob/Developing-Tool/Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MEDIA BIAS Thesis"
      ],
      "metadata": {
        "id": "5UbRZ1dHzH2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Environment"
      ],
      "metadata": {
        "id": "9DnFpdsboI45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PIP install"
      ],
      "metadata": {
        "id": "Pk8TPLKJoaiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install requests-html"
      ],
      "metadata": {
        "id": "qhBA8KY9oipI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c634ed-6330-4d62-8d39-25140e7faf91"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests-html in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.10/dist-packages (from requests-html) (1.1.3)\n",
            "Requirement already satisfied: pyquery in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.29.0)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (from requests-html) (0.0.1)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from requests-html) (1.0.2)\n",
            "Requirement already satisfied: w3lib in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.1.1)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.10/dist-packages (from requests-html) (1.19.0)\n",
            "Requirement already satisfied: certifi>=2021 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.26.15)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (6.6.0)\n",
            "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.2.2)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (10.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests-html) (4.11.2)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests-html) (4.9.2)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests-html) (1.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html) (2.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests-html) (2.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "4BDsarMioPoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "cDvttYykhT5g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Ubuntu no longer distributes chromium-browser outside of snap\n",
        "#\n",
        "# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n",
        "\n",
        "# Add debian buster\n",
        "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "EOF\n",
        "\n",
        "# Add keys\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# Prefer debian repo for chromium* packages only\n",
        "# Note the double-blank lines between entries\n",
        "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
        "Package: *\n",
        "Pin: release a=eoan\n",
        "Pin-Priority: 500\n",
        "\n",
        "\n",
        "Package: *\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 300\n",
        "\n",
        "\n",
        "Package: chromium*\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 700\n",
        "EOF\n",
        "\n",
        "# Install chromium and chromium-driver\n",
        "apt-get update\n",
        "apt-get install chromium\n",
        "\n",
        "# Install xvfb\n",
        "apt install -y xvfb\n",
        "\n",
        "# Install Selenium-Profiles\n",
        "pip uninstall -y selenium_profiles\n",
        "pip install --no-cache-dir selenium_profiles>=2.2.6\n",
        "\n",
        "# pip install https://github.com/kaliiiiiiiiii/Selenium-Profiles/archive/refs/heads/dev.zip # dev-branch\n",
        "\n",
        "# install python packages\n",
        "pip install google-colab-shell\n",
        "pip install webdriver-manager\n",
        "pip install Pyvirtualdisplay "
      ],
      "metadata": {
        "id": "8WwnQpm7DWhS",
        "outputId": "e9255f44-4cde-401e-c3b6-90d2f0c6251b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing: /tmp/apt-key-gpghome.jiM645IUod/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
            "gpg: key DCC9EFBF77E11517: \"Debian Stable Release Key (10/buster) <debian-release@lists.debian.org>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "Executing: /tmp/apt-key-gpghome.njZxFU0QhZ/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
            "gpg: key DC30D7C23CBBABEE: \"Debian Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "Executing: /tmp/apt-key-gpghome.4zdjUFX9HI/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
            "gpg: key 4DFAB270CAA96DFA: \"Debian Security Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "gpg: [stdout]: write error: Broken pipe\n",
            "gpg: filter_flush failed on close: Broken pipe\n",
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "gpg: [stdout]: write error: Broken pipe\n",
            "gpg: filter_flush failed on close: Broken pipe\n",
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "gpg: [stdout]: write error: Broken pipe\n",
            "gpg: filter_flush failed on close: Broken pipe\n",
            "Hit:1 http://deb.debian.org/debian buster InRelease\n",
            "Hit:2 http://deb.debian.org/debian buster-updates InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:5 http://deb.debian.org/debian-security buster/updates InRelease\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:14 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium is already the newest version (90.0.4430.212-1~deb10u1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Found existing installation: selenium-profiles 2.2.6\n",
            "Uninstalling selenium-profiles-2.2.6:\n",
            "  Successfully uninstalled selenium-profiles-2.2.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google-colab-shell in /usr/local/lib/python3.10/dist-packages (0.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.10/dist-packages (3.8.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (2.29.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (4.65.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (23.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iTwWZbPg1TVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1672b7c-0302-46b0-8d35-88074205aae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trio/_core/_multierror.py:406: RuntimeWarning: IPython detected, but you already have a custom exception handler installed. I'll skip installing Trio's custom handler, but this means exception groups will not show full tracebacks.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import urllib.request,sys,time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "from requests_html import HTMLSession "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting to GDrive"
      ],
      "metadata": {
        "id": "fygyA3BsoSRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "H7uTO8KVI0HL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c5d21a-5106-433f-d154-737378880c22"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dir variable"
      ],
      "metadata": {
        "id": "_CJm56YhoVDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir  = \"/content/gdrive/MyDrive/THESIS-MS/Git-Thesis22-23/\""
      ],
      "metadata": {
        "id": "FcAOB3l1JQD9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping"
      ],
      "metadata": {
        "id": "8IhlJmBFI7YI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. GMA7 - infinite scroll but with dates\n",
        "2. ABS CBN (Online) - has mix of Filipino news\n",
        "3. CNN Philippines \n",
        "4. Philippine Star \n",
        "5. Rappler\n",
        "6. Manila Bulletin - \n",
        "  (infinite more)\n",
        "7. TV5\n",
        "8. Manila Standard\n",
        "9. Sunstar Philippines"
      ],
      "metadata": {
        "id": "addLpW_M8dFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Source Function"
      ],
      "metadata": {
        "id": "386CSzX1nloW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_source(url):\n",
        "    agent = {\"User-Agent\":\"Chrome/105.0.0.0\"}\n",
        "    try:\n",
        "      source=requests.get(url, headers=agent)\n",
        "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "      error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "      print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
        "      print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
        "    \n",
        "    return source                                               #ignore this page. Abandon this and go back."
      ],
      "metadata": {
        "id": "pWGtJbU6MOyL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chrome driver"
      ],
      "metadata": {
        "id": "LSQE9C7pEWTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://colab.research.google.com/github/kaliiiiiiiiii/Selenium-Profiles/blob/master/google-colab/selenium_profiles.ipynb#scrollTo=lThF-0LvpZf3\n",
        "## @title Start actual driver\n",
        "from selenium_profiles.webdriver import Chrome\n",
        "from selenium_profiles.profiles import profiles\n",
        "from selenium.webdriver.common.by import By  # locate elements\n",
        "from selenium_profiles.utils.colab_utils import display, showscreen, show_html # virtual display\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException, ElementNotVisibleException, ElementNotSelectableException\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "def startChromeDriver():\n",
        "  options = Options()\n",
        "  # options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "  chromedriver_path = ChromeDriverManager(version=\"90.0.4430.24\").install()\n",
        "  profile = profiles.Windows() # or .Android\n",
        "  profile[\"cdp\"][\"cores\"] = None # Chrome 90 doesn't allow emulating cores :(driver = mydriver.start(profile, uc_driver=False, executable_path=chromedriver_path)\n",
        "  mydriver = Chrome(profile, executable_path=chromedriver_path, options=options)\n",
        "  # display = display()\n",
        "  # display.start_display()\n",
        "  return mydriver.start()\n",
        "   "
      ],
      "metadata": {
        "id": "RoBLs0-FH8DB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manila Bulletin"
      ],
      "metadata": {
        "id": "Hi1eB--NnqyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.scrapingbee.com/webscraping-questions/selenium/how-to-scroll-to-element-selenium/#:~:text=You%20can%20scroll%20to%20an,the%20execute_script%20as%20an%20argument."
      ],
      "metadata": {
        "id": "JvkFNSq_LpjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Manila bulletin\n",
        "# News link getter for National News\n",
        "# Simulate clicking the more button in MB site, and then extract all the news link\n",
        "\n",
        "site = \"manilaBulletin\"\n",
        "data=pd.DataFrame(columns=['Statement','Link','Date'])\n",
        "url = 'https://mb.com.ph/category/national'\n",
        "print(url)\n",
        "\n",
        "driver = startChromeDriver()\n",
        "driver.get(url)\n",
        "driver.implicitly_wait(10)\n",
        "moreClicks = 0\n",
        "for x in range(moreClicks):\n",
        "  try:\n",
        "    js_code = \"arguments[0].scrollIntoView();\"\n",
        "    # The WebElement you want to scroll to\n",
        "    # element = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div/main/div/div/div/div[3]/div[2]/div[2]/div/div[2]/button')\n",
        "    element = WebDriverWait(driver, timeout=5).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"app\"]/div/main/div/div/div/div[3]/div[2]/div[2]/div/div[2]/button')))\n",
        "    # Execute the JS script\n",
        "    driver.execute_script(js_code, element)\n",
        "    element.click()\n",
        "    print(x,'clicked')\n",
        "    # showscreen(driver)\n",
        "    # time.sleep(2)  \n",
        "  except Exception as e:                                   \n",
        "      error_type, error_obj, error_info = sys.exc_info()      \n",
        "      print ('ERROR FOR LINK:',url)                          \n",
        "      print (error_type, 'Line:', error_info.tb_lineno)  \n",
        "\n",
        "source = driver.page_source\n",
        "\n",
        "soup = BeautifulSoup(source, 'html.parser')\n",
        "links=soup.find(\"div\", {'class' : 'article-list mx-auto'}).find_all('div',attrs={'class':'row mb-5'})\n",
        "print(len(links))\n",
        "\n",
        "filename= f\"{dir}{site}_{date.today()}_NEWS_LinkList.csv\"    \n",
        "\n",
        "for j in links:\n",
        "    Statement = j.find('h4', {'class' : 'mb-font-article-title mt-0 mb-1'}).find('a').text.strip()\n",
        "    Link = \"https://mb.com.ph\"+j.find(\"h4\", {'class' : 'mb-font-article-title mt-0 mb-1'}).find('a')['href'].strip()\n",
        "    Date = j.find('div', {'class': 'ml-2'}).text.strip()\n",
        "    df_new_row = { 'Statement': Statement, 'Link': Link, 'Date': Date}\n",
        "    data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
        "\n",
        "driver.quit()\n",
        "# display.stop_display()\n",
        "data.to_csv(filename)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "qQegsdzKBVWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "L1Se0iWFLTEn",
        "outputId": "47f12cf5-3189-4db9-b2ad-1612086e54b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Statement  \\\n",
              "0    Japan ready to help PH education programs — VP...   \n",
              "1    Metro Manila Covid-19 positivity rate may reac...   \n",
              "2        Grand, Mega Lotto jackpots still up for grabs   \n",
              "3    Japanese minister tells VP Duterte: More jobs ...   \n",
              "4    Enforce ample measures vs El Niño: Abalos orde...   \n",
              "..                                                 ...   \n",
              "495  Finally! DPWH moves to relocate electric posts...   \n",
              "496  DOH records 3,148 new Covid-19 cases in the pa...   \n",
              "497  CSC issues rule on authority to dismiss 'defec...   \n",
              "498  DICT assessing possible SIM registration exten...   \n",
              "499  Here's why Senate sugar smuggling probe won't ...   \n",
              "\n",
              "                                                  Link         Date  \n",
              "0    https://mb.com.ph/2023/5/3/japan-ready-to-help...  2 hours ago  \n",
              "1    https://mb.com.ph/2023/5/3/metro-manila-covid-...  3 hours ago  \n",
              "2    https://mb.com.ph/2023/5/3/grand-mega-lotto-ja...  3 hours ago  \n",
              "3    https://mb.com.ph/2023/5/3/japanese-minister-t...  4 hours ago  \n",
              "4    https://mb.com.ph/2023/5/3/enforce-ample-measu...  4 hours ago  \n",
              "..                                                 ...          ...  \n",
              "495  https://mb.com.ph/2023/4/24/finally-dpwh-moves...   9 days ago  \n",
              "496  https://mb.com.ph/2023/4/24/doh-records-3-148-...   9 days ago  \n",
              "497  https://mb.com.ph/2023/4/24/csc-issues-rule-on...   9 days ago  \n",
              "498  https://mb.com.ph/2023/4/24/dict-assessing-pos...   9 days ago  \n",
              "499  https://mb.com.ph/2023/4/24/here-s-why-senate-...   9 days ago  \n",
              "\n",
              "[500 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a3dc38e-7f42-4955-93c4-f4aadef72d9f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Statement</th>\n",
              "      <th>Link</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Japan ready to help PH education programs — VP...</td>\n",
              "      <td>https://mb.com.ph/2023/5/3/japan-ready-to-help...</td>\n",
              "      <td>2 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Metro Manila Covid-19 positivity rate may reac...</td>\n",
              "      <td>https://mb.com.ph/2023/5/3/metro-manila-covid-...</td>\n",
              "      <td>3 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Grand, Mega Lotto jackpots still up for grabs</td>\n",
              "      <td>https://mb.com.ph/2023/5/3/grand-mega-lotto-ja...</td>\n",
              "      <td>3 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Japanese minister tells VP Duterte: More jobs ...</td>\n",
              "      <td>https://mb.com.ph/2023/5/3/japanese-minister-t...</td>\n",
              "      <td>4 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Enforce ample measures vs El Niño: Abalos orde...</td>\n",
              "      <td>https://mb.com.ph/2023/5/3/enforce-ample-measu...</td>\n",
              "      <td>4 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>Finally! DPWH moves to relocate electric posts...</td>\n",
              "      <td>https://mb.com.ph/2023/4/24/finally-dpwh-moves...</td>\n",
              "      <td>9 days ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>DOH records 3,148 new Covid-19 cases in the pa...</td>\n",
              "      <td>https://mb.com.ph/2023/4/24/doh-records-3-148-...</td>\n",
              "      <td>9 days ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>CSC issues rule on authority to dismiss 'defec...</td>\n",
              "      <td>https://mb.com.ph/2023/4/24/csc-issues-rule-on...</td>\n",
              "      <td>9 days ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>DICT assessing possible SIM registration exten...</td>\n",
              "      <td>https://mb.com.ph/2023/4/24/dict-assessing-pos...</td>\n",
              "      <td>9 days ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>Here's why Senate sugar smuggling probe won't ...</td>\n",
              "      <td>https://mb.com.ph/2023/4/24/here-s-why-senate-...</td>\n",
              "      <td>9 days ago</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a3dc38e-7f42-4955-93c4-f4aadef72d9f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1a3dc38e-7f42-4955-93c4-f4aadef72d9f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1a3dc38e-7f42-4955-93c4-f4aadef72d9f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Article Reader from LINK\n",
        "data2 = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "filename= f\"{dir}{site}_{date.today()}_NEWS.csv\"\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  url = row[\"Link\"] \n",
        "  print(url)\n",
        "  time.sleep(1)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "  text=soup.find(\"div\", {'class' : 'col-md-8 col-xl-8 col-12'})  \n",
        "  row=[]\n",
        "  try:\n",
        "    Title = text.find('h1',{'class':'pt-3 mb-font-article-title'}).text.strip()\n",
        "    Author = text.find('div',attrs={'class':'mb-font-author-name overflow-nowrap'}).find('a').text.strip()\n",
        "    Date = text.find('div', attrs={'class': 'pt-0'}).text[:-8]\n",
        "  except Exception as e:\n",
        "      pass\n",
        "  body = \"\"\n",
        "  textList =  soup.find('div', {'class':'pt-8 custom-article-body mb-font-article-body'}).find_all('p')\n",
        "  for t in textList:\n",
        "    body += (t.text.replace('\\u200b', '')) +\"\\n\" \n",
        "  row.extend((Title, Author, Date, body))\n",
        "  data2 = data2.append(pd.Series(row, index=data2.columns[:len(row)]), ignore_index=True)\n",
        "  print(index)\n",
        "data2.to_csv(filename)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "5Vt_oQKsLB7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ABS CBN"
      ],
      "metadata": {
        "id": "00MJ5PJZn6M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ABS CBN\n",
        "# pagesToGet= 2\n",
        "# topic = \"Queen Elizabeth Death\"\n",
        "site = \"ABS-CBN_NEWS\"\n",
        "\n",
        "\n",
        "data=pd.DataFrame(columns=['Title','Link','Date'])\n",
        "\n",
        "for page in range(1,pagesToGet+1):\n",
        "  url = 'https://news.abs-cbn.com/special-pages/search?q='+topic.replace(\" \", \"%20\")+'&gsc.sort=&gsc.page='+str(page)+'#gsc.tab=0&gsc.q='+topic.replace(\" \", \"%20\")+'&gsc.sort=&gsc.page='+str(page)\n",
        "  print(url)\n",
        "\n",
        "  driver.get(url)\n",
        "  # WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.ID, '___gcse_0'))) \n",
        "  source = driver.page_source\n",
        "\n",
        "  time.sleep(2)   \n",
        "  soup = BeautifulSoup(source, 'html.parser')\n",
        "  searches = soup.find_all('div', {'class':'gsc-webResult gsc-result'})\n",
        "  filename= f\"{dir}{site}-{topic}-NEWS-List.csv\"     \n",
        "\n",
        "  for x in searches:\n",
        "    frame=[]\n",
        "    Title = x.find('div', class_=\"gs-title\").find(\"a\").text.strip()\n",
        "    Date = x.find('div', class_=\"gs-bidi-start-align gs-snippet\").text[:12].strip()\n",
        "    Link = x.find('a', class_=\"gs-title\")['href']\n",
        "    frame.extend((Title, Link, Date, topic))\n",
        "\n",
        "    data = data.append(pd.Series(frame, index=data.columns[:len(frame)]), ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "data.to_csv(filename)\n",
        "driver.quit()\n",
        "data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qg8vzExe8qtP",
        "outputId": "de8a5fd8-3ed7-4cf2-d7ab-afb094b7a598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "WebDriverException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-96f31870f03c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0muserAgent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Chrome/105.0.0.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mchrome_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"user-agent={userAgent}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chromedriver'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchrome_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Link'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Topic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, service, keep_alive)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDriverFinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mDesiredCapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHROME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browserName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;34m\"goog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, port, options, service_args, desired_capabilities, service_log_path, service, keep_alive)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_process_still_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connectable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36massert_process_still_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mreturn_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mWebDriverException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Service {self._path} unexpectedly exited. Status code was: {return_code}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_connectable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWebDriverException\u001b[0m: Message: Service /usr/bin/chromedriver unexpectedly exited. Status code was: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "filename= f\"{dir+site+'-'+topic}NEWS.csv\"\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  url = row[\"Link\"] \n",
        "  if ('multimedia/photo' in url):\n",
        "    pass\n",
        "  else:\n",
        "    print(url)\n",
        "\n",
        "    time.sleep(1)   \n",
        "    soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "      \n",
        "    row=[]\n",
        "    \n",
        "    Title = soup.find('h1',{'class':'news-title'}).text.strip()\n",
        "    Author = soup.find('span',attrs={'class':'editor'}).text.strip()\n",
        "    Date = soup.find('span', attrs={'class': 'date-posted'}).text[:-8].strip() \n",
        "    \n",
        "    textList =soup.find(\"div\", {'class' :'article-content'}).find_all(\"p\")\n",
        "    body = \"\"\n",
        "    for t in textList:\n",
        "      if (t.text != \"RELATED VIDEO:\"):\n",
        "        body += (t.text) +\"\\n\" \n",
        "\n",
        "    row.extend((Title, Author, Date, body))\n",
        "    \n",
        "\n",
        "    data2 = data2.append(pd.Series(row, index=data2.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "data2.to_csv(filename)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "YIfu-vRWOzcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMA "
      ],
      "metadata": {
        "id": "WaPofL-jn9u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.gmanetwork.com/news/archives/news-nation\n",
        "#  https://stackoverflow.com/questions/20986631/how-can-i-scroll-a-web-page-using-selenium-webdriver-in-python\n",
        "site = \"GMA\"\n",
        "data=pd.DataFrame(columns=['Statement','Link','Date'])\n",
        "url = 'https://www.gmanetwork.com/news/archives/news-nation/'\n",
        "print(url)\n",
        "\n",
        "driver = startChromeDriver()\n",
        "driver.get(url)\n",
        "driver.implicitly_wait(10)\n",
        "\n",
        "try:\n",
        "  SCROLL_PAUSE_TIME = 0.5\n",
        "  cnt = 2\n",
        "  # Get scroll height\n",
        "  last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "  while cnt>0:\n",
        "      # Scroll down to bottom\n",
        "      driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "      # Wait to load page\n",
        "      time.sleep(SCROLL_PAUSE_TIME)\n",
        "      # Calculate new scroll height and compare with last scroll height\n",
        "      new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "      if new_height == last_height:\n",
        "          break\n",
        "      last_height = new_height\n",
        "      cnt -= 1\n",
        "except Exception as e:                                   \n",
        "  error_type, error_obj, error_info = sys.exc_info()      \n",
        "  print ('ERROR FOR LINK:',url)                          \n",
        "  print (error_type, 'Line:', error_info.tb_lineno)  \n",
        "\n",
        "source = driver.page_source\n",
        "soup = BeautifulSoup(source, 'html.parser')\n",
        "\n",
        "linksL=soup.find(\"ul\", {'id' : 'grid_thumbnail_stories'}).find_all('li', {'class':'story left-grid'})\n",
        "linksR=soup.find(\"ul\", {'id' : 'grid_thumbnail_stories'}).find_all('li', {'class':'story right-grid'})\n",
        "links = linksL + linksR\n",
        "print(len(links))\n",
        "\n",
        "filename= f\"{dir}{site}_{date.today()}_NEWS_LinkList.csv\"    \n",
        "\n",
        "for j in links:\n",
        "    Statement = j.find('div').find('div', {'class': 'story'}).find('div', {'class': 'story_title_holder'}).find('div', {'class' : 'story_title'}).text[:-21].strip()\n",
        "    Link = j.find('a')['href'].strip()\n",
        "    Date = j.find('div', {'class' : 'story_title_holder'}).find('div', {'class' : 'story_title'}).find('div', {'class': 'archive_date_time'}).text[:-8].strip()\n",
        "    df_new_row = { 'Statement': Statement, 'Link': Link, 'Date': Date}\n",
        "    print(Statement, Link, Date)\n",
        "    data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
        "\n",
        "driver.quit()\n",
        "data.to_csv(filename)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "SCZPr4-699Pf",
        "outputId": "a185a1c5-2f30-437e-b868-10b583d086b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.gmanetwork.com/news/archives/news-nation/\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "WebDriverException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9370e629a83d>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstartChromeDriver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-79e629d20bfe>\u001b[0m in \u001b[0;36mstartChromeDriver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;31m# display = display()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m# display.start_display()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmydriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium_profiles/webdriver.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Actual start of chrome\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, service, keep_alive)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDriverFinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mDesiredCapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHROME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browserName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;34m\"goog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, port, options, service_args, desired_capabilities, service_log_path, service, keep_alive)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             super().__init__(\n\u001b[0m\u001b[1;32m    105\u001b[0m                 command_executor=ChromiumRemoteConnection(\n\u001b[1;32m    106\u001b[0m                     \u001b[0mremote_server_addr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command_executor, desired_capabilities, browser_profile, proxy, keep_alive, file_detector, options)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authenticator_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrowser_profile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mstart_session\u001b[0;34m(self, capabilities, browser_profile)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mw3c_caps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_w3c_caps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"capabilities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw3c_caps\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEW_SESSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"sessionId\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alert\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mWebDriverException\u001b[0m: Message: unknown error: Chrome failed to start: exited abnormally.\n  (unknown error: DevToolsActivePort file doesn't exist)\n  (The process started from chrome location /usr/bin/chromium is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\nStacktrace:\n#0 0x55bb94cbbe89 <unknown>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.gmanetwork.com/news/#/search;query=queen%20elizabeth%20death;sortBy=_score;isDesc=1\n",
        "\n",
        "topic = \"Queen Elizabeth Death\"\n",
        "site = \"GMA-Network\"\n",
        "\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "userAgent = \"Chrome/105.0.0.0\"\n",
        "chrome_options.add_argument(f\"user-agent={userAgent}\")\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "data=pd.DataFrame(columns=['Title','Link','Date',\"Topic\"])\n",
        "\n",
        "url = 'https://www.gmanetwork.com/news/#/search;query='+topic.replace(\" \", \"%20\")+';sortBy=_score;isDesc=1'\n",
        "print(url)\n",
        "\n",
        "driver.get(url)\n",
        "WebDriverWait(driver, 50).until(EC.visibility_of_element_located((By.CLASS_NAME, 'clearfix'))) \n",
        "source = driver.page_source\n",
        "\n",
        "time.sleep(2)   \n",
        "soup = BeautifulSoup(source, 'html.parser')\n",
        "searches = soup.find_all('div', {'class':'zd-row-text-wrap'})\n",
        "filename= f\"{dir}{site}-{topic}-NEWS-List.csv\"     \n",
        "print(len(searches))\n",
        "\n",
        "for x in searches:\n",
        "  frame=[]\n",
        "  Title = x.find('h4', class_=\"zd-result-title ng-star-inserted\").text.strip()\n",
        "  Date = x.find('div', class_=\"zd-result-subTitle ng-star-inserted\").text[:-18].strip()\n",
        "  partialTitle = ' '.join(Title.split()[:8])\n",
        "\n",
        "  try:\n",
        "    Link = \"\"\n",
        "    to_Click = driver.find_element(By.XPATH, f\"//*[contains(text(), '{Title}')]\")\n",
        "    print(to_Click.text)\n",
        "    to_Click.click()\n",
        "    driver.switch_to.window(driver.window_handles[1])\n",
        "    Link = driver.current_url\n",
        "    driver.close()\n",
        "    driver.switch_to.window(driver.window_handles[0])\n",
        "  except Exception as e:\n",
        "    print(\"Something wrong. Error:\", e.split()[0])\n",
        "\n",
        "  frame.extend((Title, Link, Date, topic))\n",
        "\n",
        "  data = data.append(pd.Series(frame, index=data.columns[:len(frame)]), ignore_index=True)\n",
        "\n",
        "data.to_csv(filename)\n",
        "driver.quit()\n",
        "data.head(10)\n"
      ],
      "metadata": {
        "id": "RQ9n3DXbUD4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "rqq5vKh0nHZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "filename= f\"{dir+site+'-'+topic}NEWS.csv\"\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  url = row[\"Link\"] \n",
        "  if (url == \"\"):\n",
        "    continue\n",
        "  \n",
        "  print(url)\n",
        "  time.sleep(2)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "    \n",
        "  row=[]\n",
        "  \n",
        "  Title = soup.find('h1',{'class':'story_links'}).text.strip()\n",
        "  Author = soup.find('div',attrs={'class':'main-byline'}).find('div').find('div').text.strip()\n",
        "  Date = soup.find('div', attrs={'class': 'article-time'}).find('time')['datetime'][:10].strip() \n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'story_main'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    if (t.find('p', class_=\"ad\")):\n",
        "      pass\n",
        "    else:\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "\n",
        "  data2 = data2.append(pd.Series(row, index=data2.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "data2.to_csv(filename)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "yl8f_da2j_yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application"
      ],
      "metadata": {
        "id": "DhFATEIp7J54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Function"
      ],
      "metadata": {
        "id": "AAq7YZnx7E6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def abs_scrape(url):\n",
        "  abs_df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "\n",
        "  time.sleep(1)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "    \n",
        "  row=[]\n",
        "  \n",
        "  Title = soup.find('h1',{'class':'news-title'}).text.strip()\n",
        "  Author = soup.find('span',attrs={'class':'editor'}).text.strip()\n",
        "  Date = soup.find('span', attrs={'class': 'date-posted'}).text[:-8].strip() \n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'article-content'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    if (t.text != \"RELATED VIDEO:\"):\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "  abs_df = abs_df.append(pd.Series(row, index=abs_df.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "  return abs_df"
      ],
      "metadata": {
        "id": "ZYNsYAKK3cpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gma_scrape(url):\n",
        "  gma_df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "  \n",
        "  time.sleep(2)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "    \n",
        "  row=[]\n",
        "\n",
        "  Title = soup.find('h1',{'class':'story_links'}).text.strip()\n",
        "  Author = soup.find('div',attrs={'class':'main-byline'})\n",
        "  if (Author != None):\n",
        "    Author = Author.find('div').find('div').text.strip() \n",
        "  Date = soup.find('div', attrs={'class': 'article-time'}).find('time')['datetime'][:10].strip() \n",
        "\n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'story_main'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    if (t.find('p', class_=\"ad\")):\n",
        "      pass\n",
        "    else:\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "  gma_df = gma_df.append(pd.Series(row, index=gma_df.columns[:len(row)]), ignore_index=True)\n",
        "\n",
        "  return gma_df\n",
        "  \n"
      ],
      "metadata": {
        "id": "rc4Qqqk-7Pi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_scrape(url):\n",
        "\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "  userAgent = \"Chrome/105.0.0.0\"\n",
        "  chrome_options.add_argument(f\"user-agent={userAgent}\")\n",
        "  driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "  driver.get(url)\n",
        "  WebDriverWait(driver, 50).until(EC.visibility_of_element_located((By.CLASS_NAME, 'title'))) \n",
        "  source = driver.page_source\n",
        "\n",
        "  cnn_df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "  \n",
        "  soup=BeautifulSoup(source, 'html.parser')\n",
        "  # time.sleep(2)   \n",
        "\n",
        "  # print(soup)\n",
        "  row=[]\n",
        "  \n",
        "  Title = soup.find('h1',{'class':'title'}).text.strip()\n",
        "  Author = soup.find('div',attrs={'class':'author-byline'}).find('p').find('a').text.strip()\n",
        "  Date = soup.find('div', attrs={'class': 'dateLine'}).find('p', {'class': 'dateString no-icon'}).text[10:-11].strip() \n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'article-maincontent-p cnn-life-body'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "\n",
        "  cnn_df = cnn_df.append(pd.Series(row, index=cnn_df.columns[:len(row)]), ignore_index=True)\n",
        "\n",
        "  return cnn_df"
      ],
      "metadata": {
        "id": "qpXpr6qQkKfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(df, text, clean_col , sw = [] ):\n",
        "    \"\"\"\n",
        "    Remove blank texts, replaces text with lower case characters,\n",
        "    remove special characters, remove other special texts like URLs and Twitter handles, \n",
        "    remove leading and trailing whitespaces, and remove stopwords.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove blank texts\n",
        "    df[clean_col] = df[text].fillna('')     \n",
        "\n",
        "    # Transform into lowercase\n",
        "    df[clean_col] = df[clean_col].str.lower()\n",
        "\n",
        "    # # Remove non-alphanumeric characters\n",
        "    df[clean_col] = df[clean_col].str.replace(r'(@[A-Za-z0-9_]+)|([^A-Za-z0-9_ \\t])|(\\w+:\\/\\/\\S+)', '')\n",
        "    \n",
        "    # # Lemmatize verbs\n",
        "    # df['cleaned_text'] = df['cleaned_text'].apply(lambda row: ' '.join([lemmatizer.lemmatize(x, 'v') for x in row.split()]))\n",
        "\n",
        "    # # Lemmatize adjectives\n",
        "    # df['cleaned_text'] = df['cleaned_text'].apply(lambda row: ' '.join([lemmatizer.lemmatize(x, 'a') for x in row.split()]))\n",
        "\n",
        "    # # Lemmatize nouns\n",
        "    # df['cleaned_text'] = df['cleaned_text'].apply(lambda row: ' '.join([lemmatizer.lemmatize(x, 'n') for x in row.split()]))\n",
        "\n",
        "    # Remove trailing and leading whitespaces\n",
        "    df[clean_col] = df[clean_col].str.replace(r'^\\s+|\\s+$', '')\n",
        "\n",
        "    # # Remove stopwords \n",
        "    df[clean_col] = df[clean_col].apply(lambda row: ' '.join([word for word in row.split() if word not in (stopwords + sw)])) \n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "oAGB3dkZFyo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def senti_Analysis(df, col, colLabel ):\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  newcol = {'compoundScore' : [sia.polarity_scores(entry) for entry in col]}\n",
        "  if (colLabel in df.columns):\n",
        "      df[colLabel] =  newcol['compoundScore']\n",
        "  else:\n",
        "    df.insert(df.shape[1], colLabel, newcol['compoundScore'])\n",
        "  "
      ],
      "metadata": {
        "id": "b-Xwekv-LJOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape"
      ],
      "metadata": {
        "id": "nnlTbHfAQvlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "df_ = df_.append(abs_scrape(\"https://news.abs-cbn.com/news/09/29/22/ovp-eyes-coconut-palace-as-permanent-home\"), ignore_index=True)\n",
        "df_ = df_.append(gma_scrape(\"https://www.gmanetwork.com/news/topstories/nation/846433/sara-duterte-ovp-in-talks-with-gsis-for-possible-acquisition-of-coconut-palace/story/\"), ignore_index=True)\n",
        "df_.head()"
      ],
      "metadata": {
        "id": "rLC433D46a3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Oct 6"
      ],
      "metadata": {
        "id": "pNL6TdVmlv9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "abscbn = abs_scrape('https://news.abs-cbn.com/news/10/04/22/who-funded-marcos-sg-trip-not-relevant-says-bersamin')\n",
        "gma = gma_scrape('https://www.gmanetwork.com/news/topstories/nation/846954/palace-marcos-performed-job-as-president-in-singapore/story/?just_in')\n",
        "cnn = cnn_scrape('https://www.cnnphilippines.com/news/2022/10/4/Marcos-entitled-to-private-time-funding-source-irrelevant.html')\n",
        "\n",
        "df = df.append(abscbn)\n",
        "df = df.append(gma)\n",
        "df = df.append(cnn)\n"
      ],
      "metadata": {
        "id": "5CHCl7HSlz0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addedFilter = ['executive', 'president', 'trip', 'marcos', 'bersamin']\n",
        "visualize((clean(abscbn,\"Text\", \"cleanText\",addedFilter )), \"cleanText\", \"ABS-CBN\")\n",
        "visualize((clean(gma,\"Text\", \"cleanText\",addedFilter)), \"cleanText\", \"GMA\")\n",
        "visualize((clean(cnn,\"Text\", \"cleanText\", addedFilter)), \"cleanText\", \"CNN\")"
      ],
      "metadata": {
        "id": "5BcNpnAOmTVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bDJELKBYpmE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean= clean(df_, 'Text', 'clean_Textbody')\n",
        "df_clean= clean(df_, 'Title', 'clean_Title')\n",
        "df_clean[\"clean_Body_Title\"] = df_clean[\"clean_Title\"].str.cat(df_clean[\"clean_Textbody\"], sep = \"\\n\")\n",
        "df_clean.head()"
      ],
      "metadata": {
        "id": "hA-nRcFf9Bsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def POS(df, dfCol, colLabel):\n",
        "  POS = []\n",
        "  for index, row in df.iterrows():\n",
        "    # print(type(row[dfCol]), row[dfCol])\n",
        "    tokenized = sent_tokenize(row[dfCol])\n",
        "    for i in tokenized:\n",
        "      \n",
        "      # Word tokenizers is used to find the words\n",
        "      # and punctuation in a string\n",
        "      wordsList = nltk.word_tokenize(i)\n",
        "\n",
        "      wordsList = [w for w in wordsList if not w in stopwords]\n",
        "  \n",
        "      #  Using a Tagger. Which is part-of-speech\n",
        "      # tagger or POS-tagger.\n",
        "      tagged = nltk.pos_tag(wordsList)\n",
        "      POS.append(tagged)\n",
        "    df.loc[index, colLabel] =  POS\n",
        "    \n",
        " "
      ],
      "metadata": {
        "id": "Jler8A6aRQk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.geeksforgeeks.org/part-speech-tagging-stop-words-using-nltk-python/\n",
        "POS(df_clean, 'clean_Body_Title', \"POS\")\n",
        "df_clean\n"
      ],
      "metadata": {
        "id": "UwQo58BZSTbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.to_csv(f\"{dir}textCompare.csv\")"
      ],
      "metadata": {
        "id": "Mx6UTXguaEq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}