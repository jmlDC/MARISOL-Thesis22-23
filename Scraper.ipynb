{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmlDC/MediaBias-Thesis22-23/blob/Developing-Tool/Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MEDIA BIAS Thesis"
      ],
      "metadata": {
        "id": "5UbRZ1dHzH2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Environment"
      ],
      "metadata": {
        "id": "9DnFpdsboI45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PIP install"
      ],
      "metadata": {
        "id": "Pk8TPLKJoaiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install requests-html"
      ],
      "metadata": {
        "id": "qhBA8KY9oipI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84208b28-340e-4f53-b576-0474ad2f0a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Collecting w3lib\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Collecting pyquery\n",
            "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.27.1)\n",
            "Collecting pyppeteer>=0.0.14\n",
            "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parse\n",
            "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting websockets<11.0,>=10.0\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata>=1.4\n",
            "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.26.15)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
            "Collecting pyee<9.0.0,>=8.1.0\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.65.0)\n",
            "Requirement already satisfied: certifi>=2021 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (2022.12.7)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests-html) (4.11.2)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests-html) (4.9.2)\n",
            "Collecting cssselect>=1.2.0\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html) (2.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests-html) (2.4.1)\n",
            "Building wheels for collected packages: bs4, parse\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1270 sha256=8e18397db466b4ecd08eacf77cf276793e937728e5c2df6c074ab71447d71c03\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24589 sha256=d8015e543ae348c41a276a6a7e0445868bf15e330df7ee31737762c7057e7aa2\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4b/f0/eaf5a8de646d8676dc25caa01949b9f9d883b8fa2efb435bc3\n",
            "Successfully built bs4 parse\n",
            "Installing collected packages: pyee, parse, fake-useragent, websockets, w3lib, importlib-metadata, cssselect, pyquery, pyppeteer, bs4, requests-html\n",
            "Successfully installed bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.3 importlib-metadata-6.6.0 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-2.0.0 requests-html-0.10.0 w3lib-2.1.1 websockets-10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "4BDsarMioPoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "cDvttYykhT5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Ubuntu no longer distributes chromium-browser outside of snap\n",
        "#\n",
        "# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n",
        "\n",
        "# Add debian buster\n",
        "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "EOF\n",
        "\n",
        "# Add keys\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# Prefer debian repo for chromium* packages only\n",
        "# Note the double-blank lines between entries\n",
        "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
        "Package: *\n",
        "Pin: release a=eoan\n",
        "Pin-Priority: 500\n",
        "\n",
        "\n",
        "Package: *\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 300\n",
        "\n",
        "\n",
        "Package: chromium*\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 700\n",
        "EOF\n",
        "\n",
        "# Install chromium and chromium-driver\n",
        "apt-get update\n",
        "apt-get install chromium\n",
        "\n",
        "# Install xvfb\n",
        "apt install -y xvfb\n",
        "\n",
        "# Install Selenium-Profiles\n",
        "pip uninstall -y selenium_profiles\n",
        "pip install --no-cache-dir selenium_profiles>=2.2.6\n",
        "\n",
        "# pip install https://github.com/kaliiiiiiiiii/Selenium-Profiles/archive/refs/heads/dev.zip # dev-branch\n",
        "\n",
        "# install python packages\n",
        "pip install google-colab-shell\n",
        "pip install webdriver-manager\n",
        "pip install Pyvirtualdisplay "
      ],
      "metadata": {
        "id": "8WwnQpm7DWhS",
        "outputId": "f7ef07e8-8090-4165-f6d5-bf8d49a7924d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing: /tmp/apt-key-gpghome.mh5NzwwcNr/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
            "gpg: key DCC9EFBF77E11517: public key \"Debian Stable Release Key (10/buster) <debian-release@lists.debian.org>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Executing: /tmp/apt-key-gpghome.bEA7nhTqPd/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
            "gpg: key DC30D7C23CBBABEE: public key \"Debian Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Executing: /tmp/apt-key-gpghome.nyosNtr3iR/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
            "gpg: key 4DFAB270CAA96DFA: public key \"Debian Security Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "Get:1 http://deb.debian.org/debian buster InRelease [122 kB]\n",
            "Get:2 http://deb.debian.org/debian buster-updates InRelease [56.6 kB]\n",
            "Get:3 http://deb.debian.org/debian-security buster/updates InRelease [34.8 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n",
            "Get:6 http://deb.debian.org/debian buster/main amd64 Packages [10.7 MB]\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:10 http://deb.debian.org/debian buster-updates/main amd64 Packages [9,745 B]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:13 http://deb.debian.org/debian-security buster/updates/main amd64 Packages [620 kB]\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1,006 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:18 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:19 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,668 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,150 kB]\n",
            "Fetched 18.8 MB in 2s (8,531 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-common chromium-sandbox libevent-2.1-6 libfontenc1 libicu63\n",
            "  libimobiledevice6 libjpeg62-turbo libplist3 libre2-5 libu2f-udev libudev1\n",
            "  libupower-glib3 libusbmuxd6 libvpx5 libxkbfile1 libxtst6 libxxf86dga1\n",
            "  notification-daemon udev upower usbmuxd x11-utils\n",
            "Suggested packages:\n",
            "  chromium-l10n chromium-shell chromium-driver libusbmuxd-tools mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  chromium chromium-common chromium-sandbox libevent-2.1-6 libfontenc1\n",
            "  libicu63 libimobiledevice6 libjpeg62-turbo libplist3 libre2-5 libu2f-udev\n",
            "  libupower-glib3 libusbmuxd6 libvpx5 libxkbfile1 libxtst6 libxxf86dga1\n",
            "  notification-daemon udev upower usbmuxd x11-utils\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 22 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 71.5 MB of archives.\n",
            "After this operation, 253 MB of additional disk space will be used.\n",
            "Get:1 http://deb.debian.org/debian buster/main amd64 libevent-2.1-6 amd64 2.1.8-stable-4 [177 kB]\n",
            "Get:2 http://deb.debian.org/debian buster/main amd64 libicu63 amd64 63.1-6+deb10u3 [8,293 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libudev1 amd64 245.4-4ubuntu3.21 [75.9 kB]\n",
            "Get:4 http://deb.debian.org/debian buster/main amd64 libjpeg62-turbo amd64 1:1.5.2-2+deb10u1 [133 kB]\n",
            "Get:5 http://deb.debian.org/debian buster/main amd64 libvpx5 amd64 1.7.0-3+deb10u1 [800 kB]\n",
            "Get:6 http://deb.debian.org/debian buster/main amd64 chromium-common amd64 90.0.4430.212-1~deb10u1 [1,423 kB]\n",
            "Get:7 http://deb.debian.org/debian buster/main amd64 chromium amd64 90.0.4430.212-1~deb10u1 [58.3 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 udev amd64 245.4-4ubuntu3.21 [1,366 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libre2-5 amd64 20200101+dfsg-1build1 [162 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 libfontenc1 amd64 1:1.1.4-0ubuntu1 [14.0 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libxkbfile1 amd64 1:1.1.0-1 [65.3 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libxtst6 amd64 2:1.2.3-1 [12.8 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu1 [12.0 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-utils amd64 7.7+5 [199 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 libplist3 amd64 2.1.0-4build2 [31.6 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 libusbmuxd6 amd64 2.0.1-2 [19.1 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 libimobiledevice6 amd64 1.2.1~git20191129.9f79242-1build1 [65.2 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal/main amd64 libu2f-udev all 1.1.10-1 [6,108 B]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu focal/main amd64 libupower-glib3 amd64 0.99.11-1build2 [43.2 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu focal/universe amd64 notification-daemon amd64 3.20.0-4 [37.0 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu focal/main amd64 upower amd64 0.99.11-1build2 [104 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu focal/main amd64 usbmuxd amd64 1.1.1~git20191130.9af2b12-1 [38.4 kB]\n",
            "Get:23 http://deb.debian.org/debian buster/main amd64 chromium-sandbox amd64 90.0.4430.212-1~deb10u1 [146 kB]\n",
            "Fetched 71.5 MB in 2s (35.3 MB/s)\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../libudev1_245.4-4ubuntu3.21_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (245.4-4ubuntu3.21) over (245.4-4ubuntu3.19) ...\n",
            "Setting up libudev1:amd64 (245.4-4ubuntu3.21) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../00-udev_245.4-4ubuntu3.21_amd64.deb ...\n",
            "Unpacking udev (245.4-4ubuntu3.21) ...\n",
            "Selecting previously unselected package libevent-2.1-6:amd64.\n",
            "Preparing to unpack .../01-libevent-2.1-6_2.1.8-stable-4_amd64.deb ...\n",
            "Unpacking libevent-2.1-6:amd64 (2.1.8-stable-4) ...\n",
            "Selecting previously unselected package libicu63:amd64.\n",
            "Preparing to unpack .../02-libicu63_63.1-6+deb10u3_amd64.deb ...\n",
            "Unpacking libicu63:amd64 (63.1-6+deb10u3) ...\n",
            "Selecting previously unselected package libjpeg62-turbo:amd64.\n",
            "Preparing to unpack .../03-libjpeg62-turbo_1%3a1.5.2-2+deb10u1_amd64.deb ...\n",
            "Unpacking libjpeg62-turbo:amd64 (1:1.5.2-2+deb10u1) ...\n",
            "Selecting previously unselected package libre2-5:amd64.\n",
            "Preparing to unpack .../04-libre2-5_20200101+dfsg-1build1_amd64.deb ...\n",
            "Unpacking libre2-5:amd64 (20200101+dfsg-1build1) ...\n",
            "Selecting previously unselected package libvpx5:amd64.\n",
            "Preparing to unpack .../05-libvpx5_1.7.0-3+deb10u1_amd64.deb ...\n",
            "Unpacking libvpx5:amd64 (1.7.0-3+deb10u1) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../06-libfontenc1_1%3a1.1.4-0ubuntu1_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../07-libxkbfile1_1%3a1.1.0-1_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../08-libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../09-libxxf86dga1_2%3a1.1.5-0ubuntu1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../10-x11-utils_7.7+5_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5) ...\n",
            "Selecting previously unselected package chromium-common.\n",
            "Preparing to unpack .../11-chromium-common_90.0.4430.212-1~deb10u1_amd64.deb ...\n",
            "Unpacking chromium-common (90.0.4430.212-1~deb10u1) ...\n",
            "Selecting previously unselected package chromium.\n",
            "Preparing to unpack .../12-chromium_90.0.4430.212-1~deb10u1_amd64.deb ...\n",
            "Unpacking chromium (90.0.4430.212-1~deb10u1) ...\n",
            "Selecting previously unselected package chromium-sandbox.\n",
            "Preparing to unpack .../13-chromium-sandbox_90.0.4430.212-1~deb10u1_amd64.deb ...\n",
            "Unpacking chromium-sandbox (90.0.4430.212-1~deb10u1) ...\n",
            "Selecting previously unselected package libplist3:amd64.\n",
            "Preparing to unpack .../14-libplist3_2.1.0-4build2_amd64.deb ...\n",
            "Unpacking libplist3:amd64 (2.1.0-4build2) ...\n",
            "Selecting previously unselected package libusbmuxd6:amd64.\n",
            "Preparing to unpack .../15-libusbmuxd6_2.0.1-2_amd64.deb ...\n",
            "Unpacking libusbmuxd6:amd64 (2.0.1-2) ...\n",
            "Selecting previously unselected package libimobiledevice6:amd64.\n",
            "Preparing to unpack .../16-libimobiledevice6_1.2.1~git20191129.9f79242-1build1_amd64.deb ...\n",
            "Unpacking libimobiledevice6:amd64 (1.2.1~git20191129.9f79242-1build1) ...\n",
            "Selecting previously unselected package libu2f-udev.\n",
            "Preparing to unpack .../17-libu2f-udev_1.1.10-1_all.deb ...\n",
            "Unpacking libu2f-udev (1.1.10-1) ...\n",
            "Selecting previously unselected package libupower-glib3:amd64.\n",
            "Preparing to unpack .../18-libupower-glib3_0.99.11-1build2_amd64.deb ...\n",
            "Unpacking libupower-glib3:amd64 (0.99.11-1build2) ...\n",
            "Selecting previously unselected package notification-daemon.\n",
            "Preparing to unpack .../19-notification-daemon_3.20.0-4_amd64.deb ...\n",
            "Unpacking notification-daemon (3.20.0-4) ...\n",
            "Selecting previously unselected package upower.\n",
            "Preparing to unpack .../20-upower_0.99.11-1build2_amd64.deb ...\n",
            "Unpacking upower (0.99.11-1build2) ...\n",
            "Selecting previously unselected package usbmuxd.\n",
            "Preparing to unpack .../21-usbmuxd_1.1.1~git20191130.9af2b12-1_amd64.deb ...\n",
            "Unpacking usbmuxd (1.1.1~git20191130.9af2b12-1) ...\n",
            "Setting up libplist3:amd64 (2.1.0-4build2) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n",
            "Setting up chromium-sandbox (90.0.4430.212-1~deb10u1) ...\n",
            "Setting up libicu63:amd64 (63.1-6+deb10u3) ...\n",
            "Setting up notification-daemon (3.20.0-4) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Setting up libjpeg62-turbo:amd64 (1:1.5.2-2+deb10u1) ...\n",
            "Setting up udev (245.4-4ubuntu3.21) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libevent-2.1-6:amd64 (2.1.8-stable-4) ...\n",
            "Setting up libusbmuxd6:amd64 (2.0.1-2) ...\n",
            "Setting up libupower-glib3:amd64 (0.99.11-1build2) ...\n",
            "Setting up libre2-5:amd64 (20200101+dfsg-1build1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Setting up libimobiledevice6:amd64 (1.2.1~git20191129.9f79242-1build1) ...\n",
            "Setting up libvpx5:amd64 (1.7.0-3+deb10u1) ...\n",
            "Setting up libu2f-udev (1.1.10-1) ...\n",
            "Failed to send reload request: No such file or directory\n",
            "Setting up upower (0.99.11-1build2) ...\n",
            "Setting up usbmuxd (1.1.1~git20191130.9af2b12-1) ...\n",
            "Warning: The home dir /var/lib/usbmux you specified can't be accessed: No such file or directory\n",
            "Adding system user `usbmux' (UID 105) ...\n",
            "Adding new user `usbmux' (UID 105) with group `plugdev' ...\n",
            "Not creating home directory `/var/lib/usbmux'.\n",
            "Setting up x11-utils (7.7+5) ...\n",
            "Setting up chromium-common (90.0.4430.212-1~deb10u1) ...\n",
            "Setting up chromium (90.0.4430.212-1~deb10u1) ...\n",
            "update-alternatives: using /usr/bin/chromium to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for systemd (245.4-4ubuntu3.21) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libxfont2 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libxfont2 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 7 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 7,618 kB of archives.\n",
            "After this operation, 11.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libxfont2 amd64 1:2.0.3-1 [91.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-xkb-utils amd64 7.7+5 [158 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu1 [573 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-utils amd64 1:7.7+6 [91.5 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 xserver-common all 2:1.20.13-1ubuntu1~20.04.8 [27.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 xvfb amd64 2:1.20.13-1ubuntu1~20.04.8 [780 kB]\n",
            "Fetched 7,618 kB in 1s (7,580 kB/s)\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "(Reading database ... 122868 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libxfont2_1%3a2.0.3-1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.3-1) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../1-x11-xkb-utils_7.7+5_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../2-xfonts-encodings_1%3a1.0.5-0ubuntu1_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu1) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../3-xfonts-utils_1%3a7.7+6_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../4-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../5-xserver-common_2%3a1.20.13-1ubuntu1~20.04.8_all.deb ...\n",
            "Unpacking xserver-common (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../6-xvfb_2%3a1.20.13-1ubuntu1~20.04.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up x11-xkb-utils (7.7+5) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu1) ...\n",
            "Setting up xserver-common (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.3-1) ...\n",
            "Setting up xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up xfonts-utils (1:7.7+6) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n",
            "\u001b[33mWARNING: Skipping selenium_profiles as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-colab-shell\n",
            "  Downloading google-colab-shell-0.2.tar.gz (4.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: google-colab-shell\n",
            "  Building wheel for google-colab-shell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-colab-shell: filename=google_colab_shell-0.2-py3-none-any.whl size=4124 sha256=a72f121f6a0a940a5ce42057e5a99b036ea55b5d8733554dad5d30be4121925a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/36/65/95dd4599be065418a9fe1f482674c8e716ce540f3f484681d2\n",
            "Successfully built google-colab-shell\n",
            "Installing collected packages: google-colab-shell\n",
            "Successfully installed google-colab-shell-0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.10/dist-packages (3.8.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (4.65.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (1.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (2.30.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: Pyvirtualdisplay\n",
            "Successfully installed Pyvirtualdisplay-3.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTwWZbPg1TVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e412ade-b1b1-402f-c88c-362b4e3d8b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trio/_core/_multierror.py:406: RuntimeWarning: IPython detected, but you already have a custom exception handler installed. I'll skip installing Trio's custom handler, but this means exception groups will not show full tracebacks.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import urllib.request,sys,time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "from requests_html import HTMLSession "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting to GDrive"
      ],
      "metadata": {
        "id": "fygyA3BsoSRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "H7uTO8KVI0HL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae3c623-bbc2-4706-a2cb-005da32f15bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dir variable"
      ],
      "metadata": {
        "id": "_CJm56YhoVDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir  = \"/content/gdrive/MyDrive/THESIS-MS/Git-Thesis22-23/\""
      ],
      "metadata": {
        "id": "FcAOB3l1JQD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping"
      ],
      "metadata": {
        "id": "8IhlJmBFI7YI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. GMA7 - infinite scroll but with dates\n",
        "2. ABS CBN (Online) - has mix of Filipino news\n",
        "3. CNN Philippines \n",
        "4. Philippine Star \n",
        "5. Rappler\n",
        "6. Manila Bulletin - \n",
        "  (infinite more)\n",
        "7. TV5\n",
        "8. Manila Standard\n",
        "9. Sunstar Philippines"
      ],
      "metadata": {
        "id": "addLpW_M8dFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Source Function"
      ],
      "metadata": {
        "id": "386CSzX1nloW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_source(url):\n",
        "    agent = {\"User-Agent\":\"Chrome/105.0.0.0\"}\n",
        "    try:\n",
        "      source=requests.get(url, headers=agent)\n",
        "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "      error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "      print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
        "      print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
        "    \n",
        "    return source                                               #ignore this page. Abandon this and go back."
      ],
      "metadata": {
        "id": "pWGtJbU6MOyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chrome driver"
      ],
      "metadata": {
        "id": "LSQE9C7pEWTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://colab.research.google.com/github/kaliiiiiiiiii/Selenium-Profiles/blob/master/google-colab/selenium_profiles.ipynb#scrollTo=lThF-0LvpZf3\n",
        "## @title Start actual driver\n",
        "from selenium_profiles.webdriver import Chrome\n",
        "from selenium_profiles.profiles import profiles\n",
        "from selenium.webdriver.common.by import By  # locate elements\n",
        "from selenium_profiles.utils.colab_utils import display, showscreen, show_html # virtual display\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException, ElementNotVisibleException, ElementNotSelectableException\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "def startChromeDriver():\n",
        "  options = Options()\n",
        "  # options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "  chromedriver_path = ChromeDriverManager(version=\"90.0.4430.24\").install()\n",
        "  profile = profiles.Windows() # or .Android\n",
        "  profile[\"cdp\"][\"cores\"] = None # Chrome 90 doesn't allow emulating cores :(driver = mydriver.start(profile, uc_driver=False, executable_path=chromedriver_path)\n",
        "  mydriver = Chrome(profile, executable_path=chromedriver_path, options=options)\n",
        "  display = display()\n",
        "\n",
        "  display.start_display()\n",
        "  driver = mydriver.start()\n",
        "  return driver"
      ],
      "metadata": {
        "id": "RoBLs0-FH8DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manila Bulletin"
      ],
      "metadata": {
        "id": "Hi1eB--NnqyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.scrapingbee.com/webscraping-questions/selenium/how-to-scroll-to-element-selenium/#:~:text=You%20can%20scroll%20to%20an,the%20execute_script%20as%20an%20argument."
      ],
      "metadata": {
        "id": "JvkFNSq_LpjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Manila bulletin\n",
        "# News link getter for National News\n",
        "# Simulate clicking the more button in MB site, and then extract all the news link\n",
        "\n",
        "site = \"manilaBulletin\"\n",
        "data=pd.DataFrame(columns=['Statement','Link','Date'])\n",
        "url = 'https://mb.com.ph/category/national'\n",
        "print(url)\n",
        "\n",
        "driver = startChromeDriver\n",
        "driver.get(url)\n",
        "driver.implicitly_wait(10)\n",
        "\n",
        "for x in range(100):\n",
        "  try:\n",
        "    js_code = \"arguments[0].scrollIntoView();\"\n",
        "    # The WebElement you want to scroll to\n",
        "    # element = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div/main/div/div/div/div[3]/div[2]/div[2]/div/div[2]/button')\n",
        "    element = WebDriverWait(driver, timeout=5).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"app\"]/div/main/div/div/div/div[3]/div[2]/div[2]/div/div[2]/button')))\n",
        "    # Execute the JS script\n",
        "    driver.execute_script(js_code, element)\n",
        "    element.click()\n",
        "    print(x,'clicked')\n",
        "    # showscreen(driver)\n",
        "    # time.sleep(2)  \n",
        "  except Exception as e:                                   \n",
        "      error_type, error_obj, error_info = sys.exc_info()      \n",
        "      print ('ERROR FOR LINK:',url)                          \n",
        "      print (error_type, 'Line:', error_info.tb_lineno)  \n",
        "\n",
        "source = driver.page_source\n",
        "\n",
        "soup = BeautifulSoup(source, 'html.parser')\n",
        "links=soup.find(\"div\", {'class' : 'article-list mx-auto'}).find_all('div',attrs={'class':'row mb-5'})\n",
        "print(len(links))\n",
        "\n",
        "filename= f\"{dir}{site}_{date.today()}_NEWS_LinkList.csv\"    \n",
        "\n",
        "for j in links:\n",
        "    Statement = j.find('h4', {'class' : 'mb-font-article-title mt-0 mb-1'}).find('a').text.strip()\n",
        "    Link = \"https://mb.com.ph\"+j.find(\"h4\", {'class' : 'mb-font-article-title mt-0 mb-1'}).find('a')['href'].strip()\n",
        "    Date = j.find('div', {'class': 'ml-2'}).text.strip()\n",
        "    df_new_row = { 'Statement': Statement, 'Link': Link, 'Date': Date}\n",
        "    data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
        "\n",
        "driver.quit()\n",
        "# display.stop_display()\n",
        "data.to_csv(filename)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "qQegsdzKBVWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "L1Se0iWFLTEn",
        "outputId": "47f12cf5-3189-4db9-b2ad-1612086e54b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Statement  \\\n",
              "0    Japan ready to help PH education programs — VP...   \n",
              "1    Metro Manila Covid-19 positivity rate may reac...   \n",
              "2        Grand, Mega Lotto jackpots still up for grabs   \n",
              "3    Japanese minister tells VP Duterte: More jobs ...   \n",
              "4    Enforce ample measures vs El Niño: Abalos orde...   \n",
              "..                                                 ...   \n",
              "495  Finally! DPWH moves to relocate electric posts...   \n",
              "496  DOH records 3,148 new Covid-19 cases in the pa...   \n",
              "497  CSC issues rule on authority to dismiss 'defec...   \n",
              "498  DICT assessing possible SIM registration exten...   \n",
              "499  Here's why Senate sugar smuggling probe won't ...   \n",
              "\n",
              "                                                  Link         Date  \n",
              "0    https://mb.com.ph/2023/5/3/japan-ready-to-help...  2 hours ago  \n",
              "1    https://mb.com.ph/2023/5/3/metro-manila-covid-...  3 hours ago  \n",
              "2    https://mb.com.ph/2023/5/3/grand-mega-lotto-ja...  3 hours ago  \n",
              "3    https://mb.com.ph/2023/5/3/japanese-minister-t...  4 hours ago  \n",
              "4    https://mb.com.ph/2023/5/3/enforce-ample-measu...  4 hours ago  \n",
              "..                                                 ...          ...  \n",
              "495  https://mb.com.ph/2023/4/24/finally-dpwh-moves...   9 days ago  \n",
              "496  https://mb.com.ph/2023/4/24/doh-records-3-148-...   9 days ago  \n",
              "497  https://mb.com.ph/2023/4/24/csc-issues-rule-on...   9 days ago  \n",
              "498  https://mb.com.ph/2023/4/24/dict-assessing-pos...   9 days ago  \n",
              "499  https://mb.com.ph/2023/4/24/here-s-why-senate-...   9 days ago  \n",
              "\n",
              "[500 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a3dc38e-7f42-4955-93c4-f4aadef72d9f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Statement</th>\n",
              "      <th>Link</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Japan ready to help PH education programs — VP...</td>\n",
              "      <td>https://mb.com.ph/2023/5/3/japan-ready-to-help...</td>\n",
              "      <td>2 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Metro Manila Covid-19 positivity rate may reac...</td>\n",
              "      <td>https://mb.com.ph/2023/5/3/metro-manila-covid-...</td>\n",
              "      <td>3 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Grand, Mega Lotto jackpots still up for grabs</td>\n",
              "      <td>https://mb.com.ph/2023/5/3/grand-mega-lotto-ja...</td>\n",
              "      <td>3 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Japanese minister tells VP Duterte: More jobs ...</td>\n",
              "      <td>https://mb.com.ph/2023/5/3/japanese-minister-t...</td>\n",
              "      <td>4 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Enforce ample measures vs El Niño: Abalos orde...</td>\n",
              "      <td>https://mb.com.ph/2023/5/3/enforce-ample-measu...</td>\n",
              "      <td>4 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>Finally! DPWH moves to relocate electric posts...</td>\n",
              "      <td>https://mb.com.ph/2023/4/24/finally-dpwh-moves...</td>\n",
              "      <td>9 days ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>DOH records 3,148 new Covid-19 cases in the pa...</td>\n",
              "      <td>https://mb.com.ph/2023/4/24/doh-records-3-148-...</td>\n",
              "      <td>9 days ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>CSC issues rule on authority to dismiss 'defec...</td>\n",
              "      <td>https://mb.com.ph/2023/4/24/csc-issues-rule-on...</td>\n",
              "      <td>9 days ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>DICT assessing possible SIM registration exten...</td>\n",
              "      <td>https://mb.com.ph/2023/4/24/dict-assessing-pos...</td>\n",
              "      <td>9 days ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>Here's why Senate sugar smuggling probe won't ...</td>\n",
              "      <td>https://mb.com.ph/2023/4/24/here-s-why-senate-...</td>\n",
              "      <td>9 days ago</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a3dc38e-7f42-4955-93c4-f4aadef72d9f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1a3dc38e-7f42-4955-93c4-f4aadef72d9f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1a3dc38e-7f42-4955-93c4-f4aadef72d9f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Article Reader from LINK\n",
        "data2 = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "filename= f\"{dir}{site}_{date.today()}_NEWS.csv\"\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  url = row[\"Link\"] \n",
        "  print(url)\n",
        "\n",
        "  time.sleep(1)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "\n",
        "  text=soup.find(\"div\", {'class' : 'col-md-8 col-xl-8 col-12'})\n",
        "  # print(content.prettify())\n",
        "  # textList =soup.find(\"div\", {'class' : 'the-article-content'}).find(\"section\", {'class': 'article-content'}).find_all(\"p\")\n",
        "  \n",
        "  \n",
        "  row=[]\n",
        "  try:\n",
        "    Title = text.find('h1',{'class':'pt-3 mb-font-article-title'}).text.strip()\n",
        "    Author = text.find('div',attrs={'class':'mb-font-author-name overflow-nowrap'}).find('a').text.strip()\n",
        "    Date = text.find('div', attrs={'class': 'pt-0'}).text[:-8]\n",
        "  except Exception as e:\n",
        "      pass\n",
        "  \n",
        "  body = \"\"\n",
        "  textList =  soup.find('div', {'class':'pt-8 custom-article-body mb-font-article-body'}).find_all('p')\n",
        "  for t in textList:\n",
        "    body += (t.text.replace('\\u200b', '')) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "\n",
        "  data2 = data2.append(pd.Series(row, index=data2.columns[:len(row)]), ignore_index=True)\n",
        "  print(index)\n",
        "data2.to_csv(filename)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "5Vt_oQKsLB7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ABS CBN"
      ],
      "metadata": {
        "id": "00MJ5PJZn6M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ABS CBN\n",
        "# pagesToGet= 2\n",
        "# topic = \"Queen Elizabeth Death\"\n",
        "site = \"ABS-CBN_NEWS\"\n",
        "\n",
        "\n",
        "data=pd.DataFrame(columns=['Title','Link','Date'])\n",
        "\n",
        "for page in range(1,pagesToGet+1):\n",
        "  url = 'https://news.abs-cbn.com/special-pages/search?q='+topic.replace(\" \", \"%20\")+'&gsc.sort=&gsc.page='+str(page)+'#gsc.tab=0&gsc.q='+topic.replace(\" \", \"%20\")+'&gsc.sort=&gsc.page='+str(page)\n",
        "  print(url)\n",
        "\n",
        "  driver.get(url)\n",
        "  # WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.ID, '___gcse_0'))) \n",
        "  source = driver.page_source\n",
        "\n",
        "  time.sleep(2)   \n",
        "  soup = BeautifulSoup(source, 'html.parser')\n",
        "  searches = soup.find_all('div', {'class':'gsc-webResult gsc-result'})\n",
        "  filename= f\"{dir}{site}-{topic}-NEWS-List.csv\"     \n",
        "\n",
        "  for x in searches:\n",
        "    frame=[]\n",
        "    Title = x.find('div', class_=\"gs-title\").find(\"a\").text.strip()\n",
        "    Date = x.find('div', class_=\"gs-bidi-start-align gs-snippet\").text[:12].strip()\n",
        "    Link = x.find('a', class_=\"gs-title\")['href']\n",
        "    frame.extend((Title, Link, Date, topic))\n",
        "\n",
        "    data = data.append(pd.Series(frame, index=data.columns[:len(frame)]), ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "data.to_csv(filename)\n",
        "driver.quit()\n",
        "data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qg8vzExe8qtP",
        "outputId": "de8a5fd8-3ed7-4cf2-d7ab-afb094b7a598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "WebDriverException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-96f31870f03c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0muserAgent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Chrome/105.0.0.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mchrome_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"user-agent={userAgent}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chromedriver'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchrome_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Link'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Topic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, service, keep_alive)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDriverFinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mDesiredCapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHROME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browserName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;34m\"goog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, port, options, service_args, desired_capabilities, service_log_path, service, keep_alive)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_process_still_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connectable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36massert_process_still_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mreturn_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mWebDriverException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Service {self._path} unexpectedly exited. Status code was: {return_code}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_connectable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWebDriverException\u001b[0m: Message: Service /usr/bin/chromedriver unexpectedly exited. Status code was: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "filename= f\"{dir+site+'-'+topic}NEWS.csv\"\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  url = row[\"Link\"] \n",
        "  if ('multimedia/photo' in url):\n",
        "    pass\n",
        "  else:\n",
        "    print(url)\n",
        "\n",
        "    time.sleep(1)   \n",
        "    soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "      \n",
        "    row=[]\n",
        "    \n",
        "    Title = soup.find('h1',{'class':'news-title'}).text.strip()\n",
        "    Author = soup.find('span',attrs={'class':'editor'}).text.strip()\n",
        "    Date = soup.find('span', attrs={'class': 'date-posted'}).text[:-8].strip() \n",
        "    \n",
        "    textList =soup.find(\"div\", {'class' :'article-content'}).find_all(\"p\")\n",
        "    body = \"\"\n",
        "    for t in textList:\n",
        "      if (t.text != \"RELATED VIDEO:\"):\n",
        "        body += (t.text) +\"\\n\" \n",
        "\n",
        "    row.extend((Title, Author, Date, body))\n",
        "    \n",
        "\n",
        "    data2 = data2.append(pd.Series(row, index=data2.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "data2.to_csv(filename)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "YIfu-vRWOzcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMA "
      ],
      "metadata": {
        "id": "WaPofL-jn9u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.gmanetwork.com/news/archives/news-nation\n",
        "\n",
        "site = \"GMA\"\n",
        "data=pd.DataFrame(columns=['Statement','Link','Date'])\n",
        "url = 'https://www.gmanetwork.com/news/archives/news-nation/'\n",
        "print(url)\n",
        "\n",
        "driver = startChromeDriver\n",
        "driver.get(url)\n",
        "driver.implicitly_wait(10)\n",
        "\n",
        "try:\n",
        "  js_code = \"arguments[0].scrollIntoView();\"\n",
        "  element = WebDriverWait(driver, timeout=5).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"app\"]/div/main/div/div/div/div[3]/div[2]/div[2]/div/div[2]/button')))\n",
        "  driver.execute_script(js_code, element)\n",
        "  element.click()\n",
        "  print(x,'clicked')\n",
        "except Exception as e:                                   \n",
        "  error_type, error_obj, error_info = sys.exc_info()      \n",
        "  print ('ERROR FOR LINK:',url)                          \n",
        "  print (error_type, 'Line:', error_info.tb_lineno)  \n",
        "\n",
        "source = driver.page_source\n",
        "\n",
        "soup = BeautifulSoup(source, 'html.parser')\n",
        "links=soup.find(\"div\", {'class' : 'article-list mx-auto'}).find_all('div',attrs={'class':'row mb-5'})\n",
        "print(len(links))\n",
        "\n",
        "filename= f\"{dir}{site}_{date.today()}_NEWS_LinkList.csv\"    \n",
        "\n",
        "for j in links:\n",
        "    Statement = j.find('h4', {'class' : 'mb-font-article-title mt-0 mb-1'}).find('a').text.strip()\n",
        "    Link = \"https://mb.com.ph\"+j.find(\"h4\", {'class' : 'mb-font-article-title mt-0 mb-1'}).find('a')['href'].strip()\n",
        "    Date = j.find('div', {'class': 'ml-2'}).text.strip()\n",
        "    df_new_row = { 'Statement': Statement, 'Link': Link, 'Date': Date}\n",
        "    data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
        "\n",
        "driver.quit()\n",
        "# display.stop_display()\n",
        "data.to_csv(filename)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "SCZPr4-699Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.gmanetwork.com/news/#/search;query=queen%20elizabeth%20death;sortBy=_score;isDesc=1\n",
        "\n",
        "topic = \"Queen Elizabeth Death\"\n",
        "site = \"GMA-Network\"\n",
        "\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "userAgent = \"Chrome/105.0.0.0\"\n",
        "chrome_options.add_argument(f\"user-agent={userAgent}\")\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "data=pd.DataFrame(columns=['Title','Link','Date',\"Topic\"])\n",
        "\n",
        "url = 'https://www.gmanetwork.com/news/#/search;query='+topic.replace(\" \", \"%20\")+';sortBy=_score;isDesc=1'\n",
        "print(url)\n",
        "\n",
        "driver.get(url)\n",
        "WebDriverWait(driver, 50).until(EC.visibility_of_element_located((By.CLASS_NAME, 'clearfix'))) \n",
        "source = driver.page_source\n",
        "\n",
        "time.sleep(2)   \n",
        "soup = BeautifulSoup(source, 'html.parser')\n",
        "searches = soup.find_all('div', {'class':'zd-row-text-wrap'})\n",
        "filename= f\"{dir}{site}-{topic}-NEWS-List.csv\"     \n",
        "print(len(searches))\n",
        "\n",
        "for x in searches:\n",
        "  frame=[]\n",
        "  Title = x.find('h4', class_=\"zd-result-title ng-star-inserted\").text.strip()\n",
        "  Date = x.find('div', class_=\"zd-result-subTitle ng-star-inserted\").text[:-18].strip()\n",
        "  partialTitle = ' '.join(Title.split()[:8])\n",
        "\n",
        "  try:\n",
        "    Link = \"\"\n",
        "    to_Click = driver.find_element(By.XPATH, f\"//*[contains(text(), '{Title}')]\")\n",
        "    print(to_Click.text)\n",
        "    to_Click.click()\n",
        "    driver.switch_to.window(driver.window_handles[1])\n",
        "    Link = driver.current_url\n",
        "    driver.close()\n",
        "    driver.switch_to.window(driver.window_handles[0])\n",
        "  except Exception as e:\n",
        "    print(\"Something wrong. Error:\", e.split()[0])\n",
        "\n",
        "  frame.extend((Title, Link, Date, topic))\n",
        "\n",
        "  data = data.append(pd.Series(frame, index=data.columns[:len(frame)]), ignore_index=True)\n",
        "\n",
        "data.to_csv(filename)\n",
        "driver.quit()\n",
        "data.head(10)\n"
      ],
      "metadata": {
        "id": "RQ9n3DXbUD4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "rqq5vKh0nHZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "filename= f\"{dir+site+'-'+topic}NEWS.csv\"\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  url = row[\"Link\"] \n",
        "  if (url == \"\"):\n",
        "    continue\n",
        "  \n",
        "  print(url)\n",
        "  time.sleep(2)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "    \n",
        "  row=[]\n",
        "  \n",
        "  Title = soup.find('h1',{'class':'story_links'}).text.strip()\n",
        "  Author = soup.find('div',attrs={'class':'main-byline'}).find('div').find('div').text.strip()\n",
        "  Date = soup.find('div', attrs={'class': 'article-time'}).find('time')['datetime'][:10].strip() \n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'story_main'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    if (t.find('p', class_=\"ad\")):\n",
        "      pass\n",
        "    else:\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "\n",
        "  data2 = data2.append(pd.Series(row, index=data2.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "data2.to_csv(filename)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "yl8f_da2j_yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application"
      ],
      "metadata": {
        "id": "DhFATEIp7J54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Function"
      ],
      "metadata": {
        "id": "AAq7YZnx7E6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def abs_scrape(url):\n",
        "  abs_df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "\n",
        "  time.sleep(1)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "    \n",
        "  row=[]\n",
        "  \n",
        "  Title = soup.find('h1',{'class':'news-title'}).text.strip()\n",
        "  Author = soup.find('span',attrs={'class':'editor'}).text.strip()\n",
        "  Date = soup.find('span', attrs={'class': 'date-posted'}).text[:-8].strip() \n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'article-content'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    if (t.text != \"RELATED VIDEO:\"):\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "  abs_df = abs_df.append(pd.Series(row, index=abs_df.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "  return abs_df"
      ],
      "metadata": {
        "id": "ZYNsYAKK3cpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gma_scrape(url):\n",
        "  gma_df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "  \n",
        "  time.sleep(2)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "    \n",
        "  row=[]\n",
        "\n",
        "  Title = soup.find('h1',{'class':'story_links'}).text.strip()\n",
        "  Author = soup.find('div',attrs={'class':'main-byline'})\n",
        "  if (Author != None):\n",
        "    Author = Author.find('div').find('div').text.strip() \n",
        "  Date = soup.find('div', attrs={'class': 'article-time'}).find('time')['datetime'][:10].strip() \n",
        "\n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'story_main'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    if (t.find('p', class_=\"ad\")):\n",
        "      pass\n",
        "    else:\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "  gma_df = gma_df.append(pd.Series(row, index=gma_df.columns[:len(row)]), ignore_index=True)\n",
        "\n",
        "  return gma_df\n",
        "  \n"
      ],
      "metadata": {
        "id": "rc4Qqqk-7Pi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_scrape(url):\n",
        "\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "  userAgent = \"Chrome/105.0.0.0\"\n",
        "  chrome_options.add_argument(f\"user-agent={userAgent}\")\n",
        "  driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "  driver.get(url)\n",
        "  WebDriverWait(driver, 50).until(EC.visibility_of_element_located((By.CLASS_NAME, 'title'))) \n",
        "  source = driver.page_source\n",
        "\n",
        "  cnn_df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "  \n",
        "  soup=BeautifulSoup(source, 'html.parser')\n",
        "  # time.sleep(2)   \n",
        "\n",
        "  # print(soup)\n",
        "  row=[]\n",
        "  \n",
        "  Title = soup.find('h1',{'class':'title'}).text.strip()\n",
        "  Author = soup.find('div',attrs={'class':'author-byline'}).find('p').find('a').text.strip()\n",
        "  Date = soup.find('div', attrs={'class': 'dateLine'}).find('p', {'class': 'dateString no-icon'}).text[10:-11].strip() \n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'article-maincontent-p cnn-life-body'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "\n",
        "  cnn_df = cnn_df.append(pd.Series(row, index=cnn_df.columns[:len(row)]), ignore_index=True)\n",
        "\n",
        "  return cnn_df"
      ],
      "metadata": {
        "id": "qpXpr6qQkKfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(df, text, clean_col , sw = [] ):\n",
        "    \"\"\"\n",
        "    Remove blank texts, replaces text with lower case characters,\n",
        "    remove special characters, remove other special texts like URLs and Twitter handles, \n",
        "    remove leading and trailing whitespaces, and remove stopwords.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove blank texts\n",
        "    df[clean_col] = df[text].fillna('')     \n",
        "\n",
        "    # Transform into lowercase\n",
        "    df[clean_col] = df[clean_col].str.lower()\n",
        "\n",
        "    # # Remove non-alphanumeric characters\n",
        "    df[clean_col] = df[clean_col].str.replace(r'(@[A-Za-z0-9_]+)|([^A-Za-z0-9_ \\t])|(\\w+:\\/\\/\\S+)', '')\n",
        "    \n",
        "    # # Lemmatize verbs\n",
        "    # df['cleaned_text'] = df['cleaned_text'].apply(lambda row: ' '.join([lemmatizer.lemmatize(x, 'v') for x in row.split()]))\n",
        "\n",
        "    # # Lemmatize adjectives\n",
        "    # df['cleaned_text'] = df['cleaned_text'].apply(lambda row: ' '.join([lemmatizer.lemmatize(x, 'a') for x in row.split()]))\n",
        "\n",
        "    # # Lemmatize nouns\n",
        "    # df['cleaned_text'] = df['cleaned_text'].apply(lambda row: ' '.join([lemmatizer.lemmatize(x, 'n') for x in row.split()]))\n",
        "\n",
        "    # Remove trailing and leading whitespaces\n",
        "    df[clean_col] = df[clean_col].str.replace(r'^\\s+|\\s+$', '')\n",
        "\n",
        "    # # Remove stopwords \n",
        "    df[clean_col] = df[clean_col].apply(lambda row: ' '.join([word for word in row.split() if word not in (stopwords + sw)])) \n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "oAGB3dkZFyo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def senti_Analysis(df, col, colLabel ):\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  newcol = {'compoundScore' : [sia.polarity_scores(entry) for entry in col]}\n",
        "  if (colLabel in df.columns):\n",
        "      df[colLabel] =  newcol['compoundScore']\n",
        "  else:\n",
        "    df.insert(df.shape[1], colLabel, newcol['compoundScore'])\n",
        "  "
      ],
      "metadata": {
        "id": "b-Xwekv-LJOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape"
      ],
      "metadata": {
        "id": "nnlTbHfAQvlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "df_ = df_.append(abs_scrape(\"https://news.abs-cbn.com/news/09/29/22/ovp-eyes-coconut-palace-as-permanent-home\"), ignore_index=True)\n",
        "df_ = df_.append(gma_scrape(\"https://www.gmanetwork.com/news/topstories/nation/846433/sara-duterte-ovp-in-talks-with-gsis-for-possible-acquisition-of-coconut-palace/story/\"), ignore_index=True)\n",
        "df_.head()"
      ],
      "metadata": {
        "id": "rLC433D46a3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Oct 6"
      ],
      "metadata": {
        "id": "pNL6TdVmlv9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "abscbn = abs_scrape('https://news.abs-cbn.com/news/10/04/22/who-funded-marcos-sg-trip-not-relevant-says-bersamin')\n",
        "gma = gma_scrape('https://www.gmanetwork.com/news/topstories/nation/846954/palace-marcos-performed-job-as-president-in-singapore/story/?just_in')\n",
        "cnn = cnn_scrape('https://www.cnnphilippines.com/news/2022/10/4/Marcos-entitled-to-private-time-funding-source-irrelevant.html')\n",
        "\n",
        "df = df.append(abscbn)\n",
        "df = df.append(gma)\n",
        "df = df.append(cnn)\n"
      ],
      "metadata": {
        "id": "5CHCl7HSlz0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addedFilter = ['executive', 'president', 'trip', 'marcos', 'bersamin']\n",
        "visualize((clean(abscbn,\"Text\", \"cleanText\",addedFilter )), \"cleanText\", \"ABS-CBN\")\n",
        "visualize((clean(gma,\"Text\", \"cleanText\",addedFilter)), \"cleanText\", \"GMA\")\n",
        "visualize((clean(cnn,\"Text\", \"cleanText\", addedFilter)), \"cleanText\", \"CNN\")"
      ],
      "metadata": {
        "id": "5BcNpnAOmTVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bDJELKBYpmE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean= clean(df_, 'Text', 'clean_Textbody')\n",
        "df_clean= clean(df_, 'Title', 'clean_Title')\n",
        "df_clean[\"clean_Body_Title\"] = df_clean[\"clean_Title\"].str.cat(df_clean[\"clean_Textbody\"], sep = \"\\n\")\n",
        "df_clean.head()"
      ],
      "metadata": {
        "id": "hA-nRcFf9Bsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def POS(df, dfCol, colLabel):\n",
        "  POS = []\n",
        "  for index, row in df.iterrows():\n",
        "    # print(type(row[dfCol]), row[dfCol])\n",
        "    tokenized = sent_tokenize(row[dfCol])\n",
        "    for i in tokenized:\n",
        "      \n",
        "      # Word tokenizers is used to find the words\n",
        "      # and punctuation in a string\n",
        "      wordsList = nltk.word_tokenize(i)\n",
        "\n",
        "      wordsList = [w for w in wordsList if not w in stopwords]\n",
        "  \n",
        "      #  Using a Tagger. Which is part-of-speech\n",
        "      # tagger or POS-tagger.\n",
        "      tagged = nltk.pos_tag(wordsList)\n",
        "      POS.append(tagged)\n",
        "    df.loc[index, colLabel] =  POS\n",
        "    \n",
        " "
      ],
      "metadata": {
        "id": "Jler8A6aRQk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.geeksforgeeks.org/part-speech-tagging-stop-words-using-nltk-python/\n",
        "POS(df_clean, 'clean_Body_Title', \"POS\")\n",
        "df_clean\n"
      ],
      "metadata": {
        "id": "UwQo58BZSTbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.to_csv(f\"{dir}textCompare.csv\")"
      ],
      "metadata": {
        "id": "Mx6UTXguaEq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}