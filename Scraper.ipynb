{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmlDC/MediaBias-Thesis22-23/blob/Developing-Tool/Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MEDIA BIAS Thesis"
      ],
      "metadata": {
        "id": "5UbRZ1dHzH2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Environment"
      ],
      "metadata": {
        "id": "9DnFpdsboI45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PIP install"
      ],
      "metadata": {
        "id": "Pk8TPLKJoaiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install requests-html\n",
        "# !apt update\n",
        "# !apt install chromium-chromedriver\n",
        "# !pip3 install selenium\n",
        "# !pip install selenium\n",
        "# !pip3 install webdriver-manager"
      ],
      "metadata": {
        "id": "qhBA8KY9oipI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfbf8730-8d5c-404d-e3c3-49df2582b1b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Collecting pyquery\n",
            "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyppeteer>=0.0.14\n",
            "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.27.1)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting w3lib\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Collecting parse\n",
            "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
            "Collecting importlib-metadata>=1.4\n",
            "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
            "Collecting websockets<11.0,>=10.0\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyee<9.0.0,>=8.1.0\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.26.15)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.65.0)\n",
            "Requirement already satisfied: certifi>=2021 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (2022.12.7)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests-html) (4.11.2)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests-html) (4.9.2)\n",
            "Collecting cssselect>=1.2.0\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html) (3.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests-html) (2.4.1)\n",
            "Building wheels for collected packages: bs4, parse\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1270 sha256=0123728d0b92be1af99a072df270eb49fdb903d300e45a2f812cb32f265cb294\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24589 sha256=b489f8a908e5817349b3df82db1e1222c80cbf698d0cb927fa32fa201226f5d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4b/f0/eaf5a8de646d8676dc25caa01949b9f9d883b8fa2efb435bc3\n",
            "Successfully built bs4 parse\n",
            "Installing collected packages: pyee, parse, fake-useragent, websockets, w3lib, importlib-metadata, cssselect, pyquery, pyppeteer, bs4, requests-html\n",
            "Successfully installed bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.3 importlib-metadata-6.6.0 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-2.0.0 requests-html-0.10.0 w3lib-2.1.1 websockets-10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Needed for Selenium\n",
        "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "metadata": {
        "id": "rAD7RaaP1Z3s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "4BDsarMioPoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Ubuntu no longer distributes chromium-browser outside of snap\n",
        "#\n",
        "# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n",
        "\n",
        "# Add debian buster\n",
        "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "EOF\n",
        "\n",
        "# Add keys\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# Prefer debian repo for chromium* packages only\n",
        "# Note the double-blank lines between entries\n",
        "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
        "Package: *\n",
        "Pin: release a=eoan\n",
        "Pin-Priority: 500\n",
        "\n",
        "\n",
        "Package: *\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 300\n",
        "\n",
        "\n",
        "Package: chromium*\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 700\n",
        "EOF\n",
        "\n",
        "# Install chromium and chromium-driver\n",
        "apt-get update\n",
        "apt-get install chromium\n",
        "\n",
        "# Install xvfb\n",
        "apt install -y xvfb\n",
        "\n",
        "# Install Selenium-Profiles\n",
        "pip uninstall -y selenium_profiles\n",
        "pip install --no-cache-dir selenium_profiles>=2.2.6\n",
        "\n",
        "# pip install https://github.com/kaliiiiiiiiii/Selenium-Profiles/archive/refs/heads/dev.zip # dev-branch\n",
        "\n",
        "# install python packages\n",
        "pip install google-colab-shell\n",
        "pip install webdriver-manager\n",
        "pip install Pyvirtualdisplay "
      ],
      "metadata": {
        "id": "8WwnQpm7DWhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ],
      "metadata": {
        "id": "kDHMeE4S1i2s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iTwWZbPg1TVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22977156-5168-4298-a98e-0dd84d8e169e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trio/_core/_multierror.py:406: RuntimeWarning: IPython detected, but you already have a custom exception handler installed. I'll skip installing Trio's custom handler, but this means exception groups will not show full tracebacks.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import urllib.request,sys,time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "from requests_html import HTMLSession "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting to GDrive"
      ],
      "metadata": {
        "id": "fygyA3BsoSRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "H7uTO8KVI0HL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2f698f-4a82-42d1-ddaf-76af29c442cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dir variable"
      ],
      "metadata": {
        "id": "_CJm56YhoVDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir  = \"/content/gdrive/MyDrive/THESIS-MS/Git-Thesis22-23/\""
      ],
      "metadata": {
        "id": "FcAOB3l1JQD9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping"
      ],
      "metadata": {
        "id": "8IhlJmBFI7YI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. GMA7\n",
        "2. ABS CBN (Online)\n",
        "3. CNN Philippines \n",
        "4. Philippine Star \n",
        "5. Rappler\n",
        "6. Manila Bulletin - \n",
        "  (infinite more)\n",
        "7. TV5\n",
        "8. Manila Standard\n",
        "9. Sunstar Philippines"
      ],
      "metadata": {
        "id": "addLpW_M8dFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Source Function"
      ],
      "metadata": {
        "id": "386CSzX1nloW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_source(url):\n",
        "    agent = {\"User-Agent\":\"Chrome/105.0.0.0\"}\n",
        "    try:\n",
        "      source=requests.get(url, headers=agent)\n",
        "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "      error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "      print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
        "      print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
        "    \n",
        "    return source                                               #ignore this page. Abandon this and go back."
      ],
      "metadata": {
        "id": "pWGtJbU6MOyL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chrome driver"
      ],
      "metadata": {
        "id": "LSQE9C7pEWTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://colab.research.google.com/github/kaliiiiiiiiii/Selenium-Profiles/blob/master/google-colab/selenium_profiles.ipynb#scrollTo=lThF-0LvpZf3\n",
        "## @title Start actual driver\n",
        "from selenium_profiles.webdriver import Chrome\n",
        "from selenium_profiles.profiles import profiles\n",
        "from selenium.webdriver.common.by import By  # locate elements\n",
        "from selenium_profiles.utils.colab_utils import display, showscreen, show_html # virtual display\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "options = Options()\n",
        "options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "\n",
        "\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "chromedriver_path = ChromeDriverManager(version=\"90.0.4430.24\").install()\n",
        "\n",
        "profile = profiles.Windows() # or .Android\n",
        "profile[\"cdp\"][\"cores\"] = None # Chrome 90 doesn't allow emulating cores :(driver = mydriver.start(profile, uc_driver=False, executable_path=chromedriver_path)\n",
        "\n",
        "mydriver = Chrome(profile, executable_path=chromedriver_path, options=options)\n",
        "\n",
        "display = display()\n",
        "display.start_display()\n",
        "\n",
        "driver = mydriver.start()"
      ],
      "metadata": {
        "id": "RoBLs0-FH8DB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a842096-6326-4ffb-f58b-da94cf2dd1e9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/selenium_profiles/scripts/profiles.py:394: UserWarning: exact dublicate found for --disable-blink-features=AutomationControlled, skipping\n",
            "  warnings.warn(f\"exact dublicate found for {my_option}, skipping\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Media Outlet"
      ],
      "metadata": {
        "id": "Lu9cUcmCJAOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manila Bulletin"
      ],
      "metadata": {
        "id": "Hi1eB--NnqyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Manila bulletin\n",
        "\n",
        "# News link getter for National News\n",
        "\n",
        "site = \"manilaBulletin\"\n",
        "\n",
        "\n",
        "data=pd.DataFrame(columns=['Statement','Link','Date'])\n",
        "\n",
        "\n",
        "url = 'https://mb.com.ph/category/national'\n",
        "print(url)\n",
        "\n",
        "for x in range(3):\n",
        "  try:\n",
        "  # element = WebDriverWait(driver, 10).until(\n",
        "  #     EC.presence_of_element_located((By.CLASS_NAME, \"more-btn mb-font-more-button v-btn v-btn--text theme--light v-size--default indigo--text\"))\n",
        "  #     )\n",
        "    moreButton = driver.find_element(By.CLASS_NAME, 'col-sm-6 col-md-8 col-lg-8 col-12').find_element(By.CLASS_NAME, \"text-center\").find_element(By.CLASS_NAME, \"more-btn mb-font-more-button v-btn v-btn--text theme--light v-size--default indigo--text\")\n",
        "    moreButton.click()\n",
        "    print('clicked')\n",
        "  except NoSuchElementException:\n",
        "    print('NoSuchElementException')\n",
        "  # except:\n",
        "  #   print(\"idk, help\")\n",
        "  \n",
        "  time.sleep(2)  \n",
        " \n",
        "driver.quit()\n",
        "\n",
        "soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "\n",
        "links=soup.find(\"div\", {'class' : 'article-list mx-auto'}).find_all('div',attrs={'class':'row mb-5'})\n",
        "print(len(links))\n",
        "\n",
        "filename= f\"{dir}{site}_{date.today()}_NEWS_LinkList.csv\"    \n",
        "\n",
        "for j in links:\n",
        "    Statement = j.find('h4', {'class' : 'mb-font-article-title mt-0 mb-1'}).find('a').text.strip()\n",
        "    Link = \"https://mb.com.ph\"+j.find(\"h4\", {'class' : 'mb-font-article-title mt-0 mb-1'}).find('a')['href'].strip()\n",
        "    Date = j.find('div', {'class': 'ml-2'}).text.strip()\n",
        "    df_new_row = { 'Statement': Statement, 'Link': Link, 'Date': Date}\n",
        "    data = data.append(pd.Series(df_new_row, index=data.columns[:len(df_new_row)]), ignore_index=True)\n",
        "    \n",
        "data.to_csv(filename)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "qQegsdzKBVWv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d81435ac-fbb6-4528-eeb1-38688ff1a509"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://mb.com.ph/category/national\n",
            "NoSuchElementException\n",
            "NoSuchElementException\n",
            "NoSuchElementException\n",
            "0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Statement, Link, Date]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a01321c2-d16c-40e2-ae19-edb21a8cead2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Statement</th>\n",
              "      <th>Link</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a01321c2-d16c-40e2-ae19-edb21a8cead2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a01321c2-d16c-40e2-ae19-edb21a8cead2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a01321c2-d16c-40e2-ae19-edb21a8cead2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Article Reader from LINK\n",
        "data2 = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "filename= f\"{dir+site+'-'+topic}NEWS.csv\"\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  url = row[\"Link\"] \n",
        "  print(url)\n",
        "\n",
        "  time.sleep(1)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "\n",
        "  text=soup.find(\"div\", {'class' : 'col-md-8 col-xl-8 col-12'})\n",
        "  # print(content.prettify())\n",
        "  # textList =soup.find(\"div\", {'class' : 'the-article-content'}).find(\"section\", {'class': 'article-content'}).find_all(\"p\")\n",
        "  \n",
        "  \n",
        "  row=[]\n",
        "  try:\n",
        "    Title = text.find('h1',{'class':'pt-3 mb-font-article-title'}).text.strip()\n",
        "    Author = text.find('div',attrs={'class':'mb-font-author-name overflow-nowrap'}).find('a').text.strip()\n",
        "    Date = text.find('div', attrs={'class': 'pt-0'}).text[:-8]\n",
        "  except Exception as e:\n",
        "      pass\n",
        "  \n",
        "  body = \"\"\n",
        "  textList =  soup.find('div', {'class':'pt-8 custom-article-body mb-font-article-body'}).find_all('p')\n",
        "  for t in textList:\n",
        "    body += (t.text.replace('\\u200b', '')) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "\n",
        "  data2 = data2.append(pd.Series(row, index=data2.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "data2.to_csv(filename)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "5Vt_oQKsLB7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ABS CBN"
      ],
      "metadata": {
        "id": "00MJ5PJZn6M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ABS CBN\n",
        "pagesToGet= 2\n",
        "topic = \"Queen Elizabeth Death\"\n",
        "site = \"ABS-CBN_NEWS\"\n",
        "\n",
        "\n",
        "data=pd.DataFrame(columns=['Title','Link','Date',\"Topic\"])\n",
        "\n",
        "for page in range(1,pagesToGet+1):\n",
        "  url = 'https://news.abs-cbn.com/special-pages/search?q='+topic.replace(\" \", \"%20\")+'&gsc.sort=&gsc.page='+str(page)+'#gsc.tab=0&gsc.q='+topic.replace(\" \", \"%20\")+'&gsc.sort=&gsc.page='+str(page)\n",
        "  print(url)\n",
        "\n",
        "  driver.get(url)\n",
        "  # WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.ID, '___gcse_0'))) \n",
        "  source = driver.page_source\n",
        "\n",
        "  time.sleep(2)   \n",
        "  soup = BeautifulSoup(source, 'html.parser')\n",
        "  searches = soup.find_all('div', {'class':'gsc-webResult gsc-result'})\n",
        "  filename= f\"{dir}{site}-{topic}-NEWS-List.csv\"     \n",
        "\n",
        "  for x in searches:\n",
        "    frame=[]\n",
        "    Title = x.find('div', class_=\"gs-title\").find(\"a\").text.strip()\n",
        "    Date = x.find('div', class_=\"gs-bidi-start-align gs-snippet\").text[:12].strip()\n",
        "    Link = x.find('a', class_=\"gs-title\")['href']\n",
        "    frame.extend((Title, Link, Date, topic))\n",
        "\n",
        "    data = data.append(pd.Series(frame, index=data.columns[:len(frame)]), ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "data.to_csv(filename)\n",
        "driver.quit()\n",
        "data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qg8vzExe8qtP",
        "outputId": "de8a5fd8-3ed7-4cf2-d7ab-afb094b7a598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "WebDriverException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-96f31870f03c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0muserAgent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Chrome/105.0.0.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mchrome_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"user-agent={userAgent}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chromedriver'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchrome_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Link'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Topic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, service, keep_alive)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDriverFinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mDesiredCapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHROME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browserName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;34m\"goog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, port, options, service_args, desired_capabilities, service_log_path, service, keep_alive)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_process_still_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connectable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36massert_process_still_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mreturn_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mWebDriverException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Service {self._path} unexpectedly exited. Status code was: {return_code}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_connectable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWebDriverException\u001b[0m: Message: Service /usr/bin/chromedriver unexpectedly exited. Status code was: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "filename= f\"{dir+site+'-'+topic}NEWS.csv\"\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  url = row[\"Link\"] \n",
        "  if ('multimedia/photo' in url):\n",
        "    pass\n",
        "  else:\n",
        "    print(url)\n",
        "\n",
        "    time.sleep(1)   \n",
        "    soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "      \n",
        "    row=[]\n",
        "    \n",
        "    Title = soup.find('h1',{'class':'news-title'}).text.strip()\n",
        "    Author = soup.find('span',attrs={'class':'editor'}).text.strip()\n",
        "    Date = soup.find('span', attrs={'class': 'date-posted'}).text[:-8].strip() \n",
        "    \n",
        "    textList =soup.find(\"div\", {'class' :'article-content'}).find_all(\"p\")\n",
        "    body = \"\"\n",
        "    for t in textList:\n",
        "      if (t.text != \"RELATED VIDEO:\"):\n",
        "        body += (t.text) +\"\\n\" \n",
        "\n",
        "    row.extend((Title, Author, Date, body))\n",
        "    \n",
        "\n",
        "    data2 = data2.append(pd.Series(row, index=data2.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "data2.to_csv(filename)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "YIfu-vRWOzcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMA "
      ],
      "metadata": {
        "id": "WaPofL-jn9u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.gmanetwork.com/news/#/search;query=queen%20elizabeth%20death;sortBy=_score;isDesc=1\n",
        "\n",
        "topic = \"Queen Elizabeth Death\"\n",
        "site = \"GMA-Network\"\n",
        "\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "userAgent = \"Chrome/105.0.0.0\"\n",
        "chrome_options.add_argument(f\"user-agent={userAgent}\")\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "data=pd.DataFrame(columns=['Title','Link','Date',\"Topic\"])\n",
        "\n",
        "url = 'https://www.gmanetwork.com/news/#/search;query='+topic.replace(\" \", \"%20\")+';sortBy=_score;isDesc=1'\n",
        "print(url)\n",
        "\n",
        "driver.get(url)\n",
        "WebDriverWait(driver, 50).until(EC.visibility_of_element_located((By.CLASS_NAME, 'clearfix'))) \n",
        "source = driver.page_source\n",
        "\n",
        "time.sleep(2)   \n",
        "soup = BeautifulSoup(source, 'html.parser')\n",
        "searches = soup.find_all('div', {'class':'zd-row-text-wrap'})\n",
        "filename= f\"{dir}{site}-{topic}-NEWS-List.csv\"     \n",
        "print(len(searches))\n",
        "\n",
        "for x in searches:\n",
        "  frame=[]\n",
        "  Title = x.find('h4', class_=\"zd-result-title ng-star-inserted\").text.strip()\n",
        "  Date = x.find('div', class_=\"zd-result-subTitle ng-star-inserted\").text[:-18].strip()\n",
        "  partialTitle = ' '.join(Title.split()[:8])\n",
        "\n",
        "  try:\n",
        "    Link = \"\"\n",
        "    to_Click = driver.find_element(By.XPATH, f\"//*[contains(text(), '{Title}')]\")\n",
        "    print(to_Click.text)\n",
        "    to_Click.click()\n",
        "    driver.switch_to.window(driver.window_handles[1])\n",
        "    Link = driver.current_url\n",
        "    driver.close()\n",
        "    driver.switch_to.window(driver.window_handles[0])\n",
        "  except Exception as e:\n",
        "    print(\"Something wrong. Error:\", e.split()[0])\n",
        "\n",
        "  frame.extend((Title, Link, Date, topic))\n",
        "\n",
        "  data = data.append(pd.Series(frame, index=data.columns[:len(frame)]), ignore_index=True)\n",
        "\n",
        "data.to_csv(filename)\n",
        "driver.quit()\n",
        "data.head(10)\n"
      ],
      "metadata": {
        "id": "RQ9n3DXbUD4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "rqq5vKh0nHZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "filename= f\"{dir+site+'-'+topic}NEWS.csv\"\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  url = row[\"Link\"] \n",
        "  if (url == \"\"):\n",
        "    continue\n",
        "  \n",
        "  print(url)\n",
        "  time.sleep(2)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "    \n",
        "  row=[]\n",
        "  \n",
        "  Title = soup.find('h1',{'class':'story_links'}).text.strip()\n",
        "  Author = soup.find('div',attrs={'class':'main-byline'}).find('div').find('div').text.strip()\n",
        "  Date = soup.find('div', attrs={'class': 'article-time'}).find('time')['datetime'][:10].strip() \n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'story_main'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    if (t.find('p', class_=\"ad\")):\n",
        "      pass\n",
        "    else:\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "\n",
        "  data2 = data2.append(pd.Series(row, index=data2.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "data2.to_csv(filename)\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "yl8f_da2j_yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application"
      ],
      "metadata": {
        "id": "DhFATEIp7J54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Function"
      ],
      "metadata": {
        "id": "AAq7YZnx7E6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def abs_scrape(url):\n",
        "  abs_df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "\n",
        "  time.sleep(1)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "    \n",
        "  row=[]\n",
        "  \n",
        "  Title = soup.find('h1',{'class':'news-title'}).text.strip()\n",
        "  Author = soup.find('span',attrs={'class':'editor'}).text.strip()\n",
        "  Date = soup.find('span', attrs={'class': 'date-posted'}).text[:-8].strip() \n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'article-content'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    if (t.text != \"RELATED VIDEO:\"):\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "  abs_df = abs_df.append(pd.Series(row, index=abs_df.columns[:len(row)]), ignore_index=True)\n",
        "  \n",
        "  return abs_df"
      ],
      "metadata": {
        "id": "ZYNsYAKK3cpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gma_scrape(url):\n",
        "  gma_df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "  \n",
        "  time.sleep(2)   \n",
        "  soup=BeautifulSoup(extract_source(url).text, 'html.parser')\n",
        "    \n",
        "  row=[]\n",
        "\n",
        "  Title = soup.find('h1',{'class':'story_links'}).text.strip()\n",
        "  Author = soup.find('div',attrs={'class':'main-byline'})\n",
        "  if (Author != None):\n",
        "    Author = Author.find('div').find('div').text.strip() \n",
        "  Date = soup.find('div', attrs={'class': 'article-time'}).find('time')['datetime'][:10].strip() \n",
        "\n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'story_main'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "    if (t.find('p', class_=\"ad\")):\n",
        "      pass\n",
        "    else:\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "  gma_df = gma_df.append(pd.Series(row, index=gma_df.columns[:len(row)]), ignore_index=True)\n",
        "\n",
        "  return gma_df\n",
        "  \n"
      ],
      "metadata": {
        "id": "rc4Qqqk-7Pi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_scrape(url):\n",
        "\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "  userAgent = \"Chrome/105.0.0.0\"\n",
        "  chrome_options.add_argument(f\"user-agent={userAgent}\")\n",
        "  driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "  driver.get(url)\n",
        "  WebDriverWait(driver, 50).until(EC.visibility_of_element_located((By.CLASS_NAME, 'title'))) \n",
        "  source = driver.page_source\n",
        "\n",
        "  cnn_df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "  \n",
        "  soup=BeautifulSoup(source, 'html.parser')\n",
        "  # time.sleep(2)   \n",
        "\n",
        "  # print(soup)\n",
        "  row=[]\n",
        "  \n",
        "  Title = soup.find('h1',{'class':'title'}).text.strip()\n",
        "  Author = soup.find('div',attrs={'class':'author-byline'}).find('p').find('a').text.strip()\n",
        "  Date = soup.find('div', attrs={'class': 'dateLine'}).find('p', {'class': 'dateString no-icon'}).text[10:-11].strip() \n",
        "  \n",
        "  textList =soup.find(\"div\", {'class' :'article-maincontent-p cnn-life-body'}).find_all(\"p\")\n",
        "  body = \"\"\n",
        "  for t in textList:\n",
        "      body += (t.text) +\"\\n\" \n",
        "\n",
        "  row.extend((Title, Author, Date, body))\n",
        "  \n",
        "\n",
        "  cnn_df = cnn_df.append(pd.Series(row, index=cnn_df.columns[:len(row)]), ignore_index=True)\n",
        "\n",
        "  return cnn_df"
      ],
      "metadata": {
        "id": "qpXpr6qQkKfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(df, text, clean_col , sw = [] ):\n",
        "    \"\"\"\n",
        "    Remove blank texts, replaces text with lower case characters,\n",
        "    remove special characters, remove other special texts like URLs and Twitter handles, \n",
        "    remove leading and trailing whitespaces, and remove stopwords.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove blank texts\n",
        "    df[clean_col] = df[text].fillna('')     \n",
        "\n",
        "    # Transform into lowercase\n",
        "    df[clean_col] = df[clean_col].str.lower()\n",
        "\n",
        "    # # Remove non-alphanumeric characters\n",
        "    df[clean_col] = df[clean_col].str.replace(r'(@[A-Za-z0-9_]+)|([^A-Za-z0-9_ \\t])|(\\w+:\\/\\/\\S+)', '')\n",
        "    \n",
        "    # # Lemmatize verbs\n",
        "    # df['cleaned_text'] = df['cleaned_text'].apply(lambda row: ' '.join([lemmatizer.lemmatize(x, 'v') for x in row.split()]))\n",
        "\n",
        "    # # Lemmatize adjectives\n",
        "    # df['cleaned_text'] = df['cleaned_text'].apply(lambda row: ' '.join([lemmatizer.lemmatize(x, 'a') for x in row.split()]))\n",
        "\n",
        "    # # Lemmatize nouns\n",
        "    # df['cleaned_text'] = df['cleaned_text'].apply(lambda row: ' '.join([lemmatizer.lemmatize(x, 'n') for x in row.split()]))\n",
        "\n",
        "    # Remove trailing and leading whitespaces\n",
        "    df[clean_col] = df[clean_col].str.replace(r'^\\s+|\\s+$', '')\n",
        "\n",
        "    # # Remove stopwords \n",
        "    df[clean_col] = df[clean_col].apply(lambda row: ' '.join([word for word in row.split() if word not in (stopwords + sw)])) \n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "oAGB3dkZFyo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def senti_Analysis(df, col, colLabel ):\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  newcol = {'compoundScore' : [sia.polarity_scores(entry) for entry in col]}\n",
        "  if (colLabel in df.columns):\n",
        "      df[colLabel] =  newcol['compoundScore']\n",
        "  else:\n",
        "    df.insert(df.shape[1], colLabel, newcol['compoundScore'])\n",
        "  "
      ],
      "metadata": {
        "id": "b-Xwekv-LJOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape"
      ],
      "metadata": {
        "id": "nnlTbHfAQvlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "df_ = df_.append(abs_scrape(\"https://news.abs-cbn.com/news/09/29/22/ovp-eyes-coconut-palace-as-permanent-home\"), ignore_index=True)\n",
        "df_ = df_.append(gma_scrape(\"https://www.gmanetwork.com/news/topstories/nation/846433/sara-duterte-ovp-in-talks-with-gsis-for-possible-acquisition-of-coconut-palace/story/\"), ignore_index=True)\n",
        "df_.head()"
      ],
      "metadata": {
        "id": "rLC433D46a3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Oct 6"
      ],
      "metadata": {
        "id": "pNL6TdVmlv9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=[\"Title\", \"Author\", \"Date\", \"Text\"])\n",
        "abscbn = abs_scrape('https://news.abs-cbn.com/news/10/04/22/who-funded-marcos-sg-trip-not-relevant-says-bersamin')\n",
        "gma = gma_scrape('https://www.gmanetwork.com/news/topstories/nation/846954/palace-marcos-performed-job-as-president-in-singapore/story/?just_in')\n",
        "cnn = cnn_scrape('https://www.cnnphilippines.com/news/2022/10/4/Marcos-entitled-to-private-time-funding-source-irrelevant.html')\n",
        "\n",
        "df = df.append(abscbn)\n",
        "df = df.append(gma)\n",
        "df = df.append(cnn)\n"
      ],
      "metadata": {
        "id": "5CHCl7HSlz0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addedFilter = ['executive', 'president', 'trip', 'marcos', 'bersamin']\n",
        "visualize((clean(abscbn,\"Text\", \"cleanText\",addedFilter )), \"cleanText\", \"ABS-CBN\")\n",
        "visualize((clean(gma,\"Text\", \"cleanText\",addedFilter)), \"cleanText\", \"GMA\")\n",
        "visualize((clean(cnn,\"Text\", \"cleanText\", addedFilter)), \"cleanText\", \"CNN\")"
      ],
      "metadata": {
        "id": "5BcNpnAOmTVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bDJELKBYpmE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean= clean(df_, 'Text', 'clean_Textbody')\n",
        "df_clean= clean(df_, 'Title', 'clean_Title')\n",
        "df_clean[\"clean_Body_Title\"] = df_clean[\"clean_Title\"].str.cat(df_clean[\"clean_Textbody\"], sep = \"\\n\")\n",
        "df_clean.head()"
      ],
      "metadata": {
        "id": "hA-nRcFf9Bsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def POS(df, dfCol, colLabel):\n",
        "  POS = []\n",
        "  for index, row in df.iterrows():\n",
        "    # print(type(row[dfCol]), row[dfCol])\n",
        "    tokenized = sent_tokenize(row[dfCol])\n",
        "    for i in tokenized:\n",
        "      \n",
        "      # Word tokenizers is used to find the words\n",
        "      # and punctuation in a string\n",
        "      wordsList = nltk.word_tokenize(i)\n",
        "\n",
        "      wordsList = [w for w in wordsList if not w in stopwords]\n",
        "  \n",
        "      #  Using a Tagger. Which is part-of-speech\n",
        "      # tagger or POS-tagger.\n",
        "      tagged = nltk.pos_tag(wordsList)\n",
        "      POS.append(tagged)\n",
        "    df.loc[index, colLabel] =  POS\n",
        "    \n",
        " "
      ],
      "metadata": {
        "id": "Jler8A6aRQk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.geeksforgeeks.org/part-speech-tagging-stop-words-using-nltk-python/\n",
        "POS(df_clean, 'clean_Body_Title', \"POS\")\n",
        "df_clean\n"
      ],
      "metadata": {
        "id": "UwQo58BZSTbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.to_csv(f\"{dir}textCompare.csv\")"
      ],
      "metadata": {
        "id": "Mx6UTXguaEq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}